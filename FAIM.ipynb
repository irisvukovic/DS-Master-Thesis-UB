{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FAIM"
      ],
      "metadata": {
        "id": "Qsf_imX_F4v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions from https://github.com/nliulab/FAIM"
      ],
      "metadata": {
        "id": "OTCt51QUGDsR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5prZZ2x9pWAP"
      },
      "source": [
        "## functions and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RRNOvD_pYYR"
      },
      "outputs": [],
      "source": [
        "def rgb01_hex(col):\n",
        "    col_hex = [round(i * 255) for i in col]\n",
        "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
        "    return col_hex\n",
        "\n",
        "\n",
        "def compute_area(fairness_metrics):\n",
        "    n_metric = len(fairness_metrics)\n",
        "    tmp = fairness_metrics.values.flatten().tolist()\n",
        "    tmp_1 = tmp[1:] + tmp[:1]\n",
        "\n",
        "    if n_metric > 2:\n",
        "        theta_c = 2 * np.pi / n_metric\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
        "    elif n_metric == 2:\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
        "    else:\n",
        "        area = np.abs(tmp[0])\n",
        "\n",
        "    return area\n",
        "\n",
        "\n",
        "def plot_perf_metric(\n",
        "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
        "):\n",
        "    \"\"\" Plot performance metrics of sampled models\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        perf_metric : numpy.array or pandas.Series\n",
        "            Numeric vector of performance metrics for all sampled models\n",
        "        eligible : numpy.array or pandas.Series\n",
        "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
        "                whether each sample is eligible.\n",
        "        x_range : list\n",
        "            Numeric vector indicating the range of eligible values for \\\n",
        "                performance metrics.\n",
        "            Will be indicated by dotted vertical lines in plots.\n",
        "        select : list or numpy.array, optional (default: None)\n",
        "            Numeric vector of indexes of 'perf_metric' to be selected\n",
        "        plot_selected : bool, optional (default: False)\n",
        "            Whether performance metrics of selected models should be plotted in \\\n",
        "                a secondary figure.\n",
        "        x_breaks : list, optional (default: None)\n",
        "            If selected models are to be plotted, the breaks to use in the \\\n",
        "                histogram\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        plot : plotnine.ggplot\n",
        "            Histogram(s) of model performance made using ggplot\n",
        "    \"\"\"\n",
        "    m = len(perf_metric)\n",
        "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
        "    plot = (\n",
        "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
        "        + pn.geoms.geom_histogram(\n",
        "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
        "        )\n",
        "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
        "        + pn.labels.labs(\n",
        "            x=\"Ratio of loss to minimum loss\",\n",
        "            title=\"\"\"Loss of {m:d} sampled models\n",
        "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
        "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
        "            ),\n",
        "        )\n",
        "        + pn.themes.theme_bw()\n",
        "        + pn.themes.theme(\n",
        "            title=pn.themes.element_text(ha=\"left\"),\n",
        "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
        "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
        "        )\n",
        "    )\n",
        "    if plot_selected:\n",
        "        if select is None:\n",
        "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
        "            select = [i for i in range(len(perf_df))]\n",
        "        try:\n",
        "            perf_select = perf_df.iloc[select]\n",
        "        except:\n",
        "            print(\n",
        "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
        "            )\n",
        "            select = [i for i in range(len(perf_df))]\n",
        "            perf_select = perf_df.iloc[select]\n",
        "        plot2 = (\n",
        "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
        "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
        "            + pn.labels.labs(\n",
        "                x=\"Ratio of loss to minimum loss\",\n",
        "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
        "            )\n",
        "            + pn.themes.theme_bw()\n",
        "            + pn.themes.theme(\n",
        "                title=pn.themes.element_text(ha=\"left\"),\n",
        "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
        "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
        "            )\n",
        "        )\n",
        "        return (plot, plot2)\n",
        "    else:\n",
        "        return plot\n",
        "\n",
        "\n",
        "def plot_distribution(df, s=4):\n",
        "    num_metrics = df.shape[1] - 2\n",
        "    labels = df.sen_var_exclusion.unique()\n",
        "    for i in range(len(labels)):\n",
        "        if labels[i] == \"\":\n",
        "            labels[i] = \"No exclusion\"\n",
        "        elif len(labels[i].split(\"_\")) == 2:\n",
        "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
        "        elif len(labels[i].split(\"_\")) > 2:\n",
        "            sens = labels[i].split(\"_\")\n",
        "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
        "        else:\n",
        "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
        "    for i, x in enumerate(df.columns[:-2]):\n",
        "        ax = axes[i]\n",
        "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
        "        sns.histplot(\n",
        "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
        "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
        "        ax.set_title(x)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
        "\n",
        "    plt.legend(\n",
        "        loc=\"center left\",\n",
        "        title=\"\",\n",
        "        labels=labels[::-1],\n",
        "        ncol=1,\n",
        "        bbox_to_anchor=(1.04, 0.5),\n",
        "        borderaxespad=0,\n",
        "    )\n",
        "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
        "    ### basic settings ###\n",
        "    np.random.seed(0)\n",
        "    if \"figsize\" not in kwargs.keys():\n",
        "        fig_h = 400\n",
        "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
        "    else:\n",
        "        figsize = kwargs[\"figsize\"]\n",
        "    caption_size = figsize[1] / c1  # control font size / figure size\n",
        "    fig_caption_ratio = 0.8\n",
        "    fig_font_size = caption_size * fig_caption_ratio\n",
        "\n",
        "    font_family = \"Arial\"\n",
        "    highlight_color = \"#D4AF37\"\n",
        "    fig_font_unit = c2  # control the relative position of elements\n",
        "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
        "    d = fig_font_unit / 8\n",
        "    legend_pos_y = 1 + fig_font_unit\n",
        "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
        "    xlab_pos_y = -fig_font_unit * 2\n",
        "\n",
        "    area_list = []\n",
        "    for i, id in enumerate(df.index):\n",
        "        values = df.loc[id, :]\n",
        "        area_list.append(1 / compute_area(values))\n",
        "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
        "\n",
        "    # jittering for display\n",
        "    jitter_control = np.zeros(len(ranking))\n",
        "    for idx in range(len(ranking)):\n",
        "        if ranking[idx] == 0:\n",
        "            jitter_control[idx] = 0\n",
        "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
        "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
        "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
        "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
        "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
        "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
        "        else:\n",
        "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
        "\n",
        "    ### plot ###\n",
        "    best_id = df.index[np.where(ranking == 0)][0]\n",
        "    worst_id = df.index[np.argmin(area_list)]\n",
        "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
        "\n",
        "    num_metrics = df.shape[1]\n",
        "    num_models = df.shape[0]\n",
        "\n",
        "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
        "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
        "    cmap = cmap[::-1]\n",
        "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
        "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
        "\n",
        "    shapes = sen_var_exclusion.copy().tolist()\n",
        "    cases = sen_var_exclusion.unique()\n",
        "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
        "    for i, case in enumerate(cases):\n",
        "        for j, v in enumerate(sen_var_exclusion):\n",
        "            if v == case:\n",
        "                shapes[j] = shapes_candidates[i]\n",
        "\n",
        "        if cases[i] == \"\":\n",
        "            cases[i] = \"No exclusion\"\n",
        "        elif len(cases[i].split(\"_\")) == 2:\n",
        "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
        "        elif len(cases[i].split(\"_\")) > 2:\n",
        "            sens = cases[i].split(\"_\")\n",
        "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
        "        else:\n",
        "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
        "\n",
        "    fair_index_df = pd.DataFrame(\n",
        "        {\n",
        "            \"model id\": df.index,\n",
        "            \"fair_index\": area_list,\n",
        "            \"ranking\": ranking,\n",
        "            \"eod\": df[\"Equalized Odds\"],\n",
        "            \"colors\": colors,\n",
        "            \"shapes\": shapes,\n",
        "            \"sizes\": sizes,\n",
        "            \"cases\": sen_var_exclusion,\n",
        "            \"jitter\": jitter_control,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add scatter plots to the subplots\n",
        "    for k, s in enumerate(shapes_candidates):\n",
        "        for i in range(num_metrics):\n",
        "            # index of sen_var_exclusion(shape) == s\n",
        "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
        "            x = df.iloc[s_idx, i].values\n",
        "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
        "            jittered_x = x + js\n",
        "\n",
        "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
        "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
        "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
        "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
        "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
        "            r = (\n",
        "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
        "                .apply(lambda x: math.log10(x + 1))\n",
        "                .values\n",
        "            )\n",
        "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
        "\n",
        "            hovertext = [\n",
        "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
        "                for f, x, i in zip(fair_index, rank_text, ids)\n",
        "            ]\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=r,\n",
        "                    y=jittered_x,\n",
        "                    customdata=hovertext,\n",
        "                    mode=\"markers\",\n",
        "                    marker=dict(\n",
        "                        color=col,\n",
        "                        symbol=s,\n",
        "                        size=size,\n",
        "                        line=dict(color=col, width=1),\n",
        "                        opacity=0.8,\n",
        "                    ),\n",
        "                    hovertemplate=\"%{customdata}.\",\n",
        "                    hoverlabel=None,\n",
        "                    hoverinfo=\"name+z\",\n",
        "                    name=cases[k],\n",
        "                ),\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "            )\n",
        "\n",
        "            if i == int((df.shape[1] + 0.5) / 2):\n",
        "                fig.update_xaxes(\n",
        "                    title_text=None,\n",
        "                    tickvals=[0, 1, 2, 3],\n",
        "                    ticktext=[1, 10, 100, 1000],\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                    tickangle=0,\n",
        "                )\n",
        "            else:\n",
        "                fig.update_xaxes(\n",
        "                    title_text=None,\n",
        "                    tickvals=[0, 1, 2, 3],\n",
        "                    ticktext=[1, 10, 100, 1000],\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                    tickangle=0,\n",
        "                )\n",
        "            fig.update_yaxes(\n",
        "                title_text=df.columns[i],\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "                showticksuffix=\"none\",\n",
        "                titlefont={\"size\": caption_size},\n",
        "            )\n",
        "\n",
        "            fig.add_vline(\n",
        "                x=0,\n",
        "                line_width=2,\n",
        "                line_dash=\"dot\",\n",
        "                line_color=highlight_color,\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "            )\n",
        "\n",
        "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
        "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
        "            meduim_metric = df.loc[\n",
        "                ranking == int(num_models / 2), df.columns[i]\n",
        "            ].values[0]\n",
        "\n",
        "            # add annotations\n",
        "            anno_size = caption_size * 0.7\n",
        "            if k == 0:\n",
        "                fig.add_hline(\n",
        "                    y=min_metric,\n",
        "                    line_width=2,\n",
        "                    line_dash=\"dot\",\n",
        "                    line_color=highlight_color,\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                )\n",
        "\n",
        "                # position_y = np.mean(df.iloc[:, i])\n",
        "                min_annotation = {\n",
        "                    \"x\": 0,\n",
        "                    \"y\": min_metric,\n",
        "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                }\n",
        "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
        "            if meduim_id in ids:\n",
        "                medium_annotation = {\n",
        "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
        "                    \"y\": meduim_metric + jitter_control[meduim_id],\n",
        "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                }\n",
        "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
        "            if worst_id in ids:\n",
        "                max_annotation = {\n",
        "                    \"x\": math.log10(num_models + 1),\n",
        "                    \"y\": max_metric + jitter_control[worst_id],\n",
        "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                    \"align\": \"left\",\n",
        "                }\n",
        "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
        "\n",
        "    colorbar_trace = go.Scatter(\n",
        "        x=[None],\n",
        "        y=[None],\n",
        "        mode=\"markers\",\n",
        "        hoverinfo=\"none\",\n",
        "        marker=dict(\n",
        "            colorscale=[\n",
        "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
        "                \"steelblue\",\n",
        "            ],  # \"magma\",\n",
        "            showscale=True,\n",
        "            cmin=0,\n",
        "            cmax=2,\n",
        "            colorbar=dict(\n",
        "                title=None,\n",
        "                thickness=10,\n",
        "                tickvals=[0, 2],\n",
        "                ticktext=[\"Low\", \"High\"],\n",
        "                outlinewidth=0,\n",
        "                orientation=\"v\",\n",
        "                x=1,\n",
        "                y=0.5,\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    fig.add_trace(colorbar_trace)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        font=dict(family=\"Arial\", size=fig_font_size),\n",
        "        hovermode=\"closest\",\n",
        "        width=figsize[0],\n",
        "        height=figsize[1],\n",
        "        showlegend=True,\n",
        "        template=\"simple_white\",\n",
        "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
        "    )\n",
        "\n",
        "    rectangle = {\n",
        "        \"type\": \"rect\",\n",
        "        \"x0\": -0.1,\n",
        "        \"y0\": subtitle_pos[0],\n",
        "        \"x1\": 1.1,\n",
        "        \"y1\": subtitle_pos[1],\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"fillcolor\": \"steelblue\",\n",
        "        \"opacity\": 0.1,\n",
        "    }  # 'line': {'color': 'red', 'width': 2},\n",
        "    fig.add_shape(rectangle)\n",
        "    subtitle_annotation = {\n",
        "        \"x\": -0.1,\n",
        "        \"y\": subtitle_pos[1],\n",
        "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": caption_size * 1.1},\n",
        "        \"align\": \"left\",\n",
        "    }\n",
        "    xaxis_annotation = {\n",
        "        \"x\": 0.5,\n",
        "        \"y\": xlab_pos_y,\n",
        "        \"text\": \"Model Rank\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": caption_size},\n",
        "    }\n",
        "    colorbar_title = {\n",
        "        \"x\": 1.05,\n",
        "        \"y\": 0.5,\n",
        "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": anno_size * 0.9},\n",
        "        \"textangle\": 90,\n",
        "    }\n",
        "    fig.add_annotation(subtitle_annotation)\n",
        "    fig.add_annotation(xaxis_annotation)\n",
        "    fig.add_annotation(colorbar_title)\n",
        "\n",
        "    for i, trace in enumerate(fig.data):\n",
        "        if i % num_metrics == 1:\n",
        "            trace.update(showlegend=True)\n",
        "        else:\n",
        "            trace.update(showlegend=False)\n",
        "    # fig.show()\n",
        "\n",
        "    return fig, fair_index_df\n",
        "\n",
        "\n",
        "def plot_radar(df, thresh_show, title, **kwargs):\n",
        "    fig = go.Figure()\n",
        "    # fig = sp.make_subplots(rows=1, cols=2)\n",
        "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
        "    theta = df.columns.tolist()\n",
        "    theta += theta[:1]\n",
        "    area_list = []\n",
        "\n",
        "    for i, id in enumerate(df.index):\n",
        "        values = df.loc[id, :]\n",
        "        area_list.append(compute_area(values))\n",
        "        values = values.values.flatten().tolist()\n",
        "        values += values[:1]\n",
        "        info = [\n",
        "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
        "        ]\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(\n",
        "                r=values,\n",
        "                theta=theta,\n",
        "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
        "                text=\"\\n\".join(info),\n",
        "                name=f\"{id}\",\n",
        "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    ranking = np.argsort(np.argsort(area_list))\n",
        "    best_id = df.index[np.where(ranking == 0)][0]\n",
        "    print(\n",
        "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
        "    )\n",
        "    values = df.loc[best_id, :].values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "    info = [\n",
        "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
        "    ]\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=values,\n",
        "            theta=theta,\n",
        "            fill=\"toself\",\n",
        "            text=\"\\n\".join(info),\n",
        "            name=f\"model {best_id}\",\n",
        "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        # title = title,\n",
        "        font=dict(family=\"Arial\", size=16),\n",
        "        polar=dict(\n",
        "            # bgcolor = \"#1e2130\",\n",
        "            radialaxis=dict(\n",
        "                showgrid=True,\n",
        "                gridwidth=1,\n",
        "                gridcolor=\"lightgray\",\n",
        "                visible=True,\n",
        "                range=[0, thresh_show],\n",
        "            )\n",
        "        ),\n",
        "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
        "        showlegend=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_bar(\n",
        "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
        "):\n",
        "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
        "    if \"color\" not in kwargs.keys():\n",
        "        color = \"steelblue\"\n",
        "    else:\n",
        "        color = kwargs[\"color\"]\n",
        "\n",
        "    def get_prefix(v):\n",
        "        if \"_\" in v and (v not in original_feature_names):\n",
        "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
        "            return [s for s in tmp if s in original_feature_names][0]\n",
        "        else:\n",
        "            return v\n",
        "\n",
        "    if shap_values is not None:\n",
        "\n",
        "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
        "            by=get_prefix, axis=0\n",
        "        )\n",
        "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
        "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
        "        df.columns = [\"Var\", \"Value\"]\n",
        "\n",
        "        df = pd.concat(\n",
        "            [\n",
        "                df,\n",
        "                pd.DataFrame(\n",
        "                    {\n",
        "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
        "                        \"order\": np.abs(df.Value),\n",
        "                    }\n",
        "                ),\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "\n",
        "    elif coef is not None:\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"Var\": coef.index,\n",
        "                \"Value\": coef.values,\n",
        "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
        "                \"order\": np.abs(coef.values),\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
        "\n",
        "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
        "    df = df.sort_values(by=\"order\", ascending=True)\n",
        "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
        "\n",
        "    common_theme = theme(\n",
        "        text=element_text(size=24),\n",
        "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
        "        panel_grid_minor=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        axis_line_x=element_line(colour=\"black\"),\n",
        "        axis_ticks_major_y=element_blank(),\n",
        "    )\n",
        "\n",
        "    x_lab = \"Feature importance\"\n",
        "\n",
        "    p = (\n",
        "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
        "        + geom_hline(yintercept=0, color=\"grey\")\n",
        "        + geom_bar(stat=\"identity\")\n",
        "        + common_theme\n",
        "        + coord_flip()\n",
        "        + labs(x=\"\", y=x_lab, title=title)\n",
        "        + theme(legend_position=\"none\")\n",
        "        + scale_fill_manual(values=[color])\n",
        "    )\n",
        "    return p\n",
        "\n",
        "\n",
        "\n",
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "rng = np.random.RandomState(seed)\n",
        "\n",
        "\n",
        "# metrics\n",
        "def get_ci_auc(y_true, y_pred, alpha=0.05, type=\"auc\"):\n",
        "    \"\"\"Calculate the confidence interval for the AUC (Area Under the Curve) score\n",
        "    or PR (Precision-Recall) score using bootstrapping.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True labels.\n",
        "        y_pred (array-like): Predicted scores or probabilities.\n",
        "        alpha (float, optional): Significance level for the confidence interval. Default is 0.05.\n",
        "        type (str, optional): Type of score to calculate: 'auc' (default) or 'pr' (precision-recall).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "\n",
        "    n_bootstraps = 1000\n",
        "    bootstrapped_scores = []\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # bootstrap by sampling with replacement on the prediction indices\n",
        "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
        "\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "\n",
        "        if type == \"pr\":\n",
        "            precision, recall, thresholds = precision_recall_curve(\n",
        "                y_true[indices], y_pred[indices]\n",
        "            )\n",
        "            score = auc(recall, precision)\n",
        "        else:\n",
        "            score = roc_auc_score(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    sorted_scores = np.array(bootstrapped_scores)\n",
        "    sorted_scores.sort()\n",
        "\n",
        "    # 95% c.i.\n",
        "    confidence_lower = sorted_scores[int(alpha / 2 * len(sorted_scores))]\n",
        "    confidence_upper = sorted_scores[int(1 - alpha / 2 * len(sorted_scores))]\n",
        "\n",
        "    return confidence_lower, np.median(sorted_scores), confidence_upper\n",
        "\n",
        "\n",
        "def find_optimal_cutoff(target, predicted, method=\"auc\"):\n",
        "    \"\"\"Find the optimal probability cutoff point for a classification model related to event rate.\n",
        "\n",
        "    Args:\n",
        "        target (array-like): True labels.\n",
        "        predicted (array-like): Predicted scores or probabilities.\n",
        "        method (str, optional): Method for finding the optimal cutoff. Default is 'auc'.\n",
        "\n",
        "    Returns:\n",
        "        list: List of optimal cutoff values.\n",
        "    \"\"\"\n",
        "    if method == \"auc\":\n",
        "        fpr, tpr, threshold = roc_curve(target, predicted)\n",
        "        i = np.arange(len(tpr))\n",
        "        roc = pd.DataFrame(\n",
        "            {\n",
        "                \"tf\": pd.Series(tpr + (1 - fpr), index=i),\n",
        "                \"threshold\": pd.Series(threshold, index=i),\n",
        "            }\n",
        "        )\n",
        "        roc_t = roc.iloc[(roc.tf - 0).abs().argsort()[::-1][:1]]\n",
        "    elif method == \"pr-auc\":\n",
        "        precision, recall, threshold = precision_recall_curve(target, predicted)\n",
        "        i = np.arange(len(precision))\n",
        "        prc = pd.DataFrame(\n",
        "            {\n",
        "                \"tf\": pd.Series(tpr - (1 - fpr), index=i),\n",
        "                \"threshold\": pd.Series(threshold, index=i),\n",
        "            }\n",
        "        )\n",
        "        prc_t = prc.iloc[(prc.tf - 0).abs().argsort()[:1]]\n",
        "\n",
        "    return list(roc_t[\"threshold\"])\n",
        "\n",
        "\n",
        "def get_cal_fairness(df):\n",
        "    def absolute_difference(x):\n",
        "        return np.abs(spline1(x) - spline0(x))\n",
        "\n",
        "    df.groupby(\"group\").apply(lambda x: np.max(x[\"p_obs\"]))\n",
        "    # for g in df_calib.group.unique():\n",
        "\n",
        "    gs = df.group.unique()\n",
        "    pairs = list(combinations(gs, 2))\n",
        "\n",
        "    x_min_thresh = np.min(df.groupby(\"group\").apply(lambda x: np.min(x[\"p_pred\"])))\n",
        "    x_max_thresh = np.max(df.groupby(\"group\").apply(lambda x: np.max(x[\"p_pred\"])))\n",
        "    num_points = 100\n",
        "    diff_cal = []\n",
        "\n",
        "    for p in pairs:\n",
        "        p0 = df.loc[df.group == p[0], [\"p_obs\", \"p_pred\"]].sort_values(\n",
        "            by=\"p_pred\", ascending=True\n",
        "        )\n",
        "        p1 = df.loc[df.group == p[1], [\"p_obs\", \"p_pred\"]].sort_values(\n",
        "            by=\"p_pred\", ascending=True\n",
        "        )\n",
        "\n",
        "        x0 = p0[\"p_pred\"]\n",
        "        y0 = p0[\"p_obs\"]\n",
        "        spline0 = CubicSpline(x0, y0)\n",
        "\n",
        "        x1 = p1[\"p_pred\"]\n",
        "        y1 = p1[\"p_obs\"]\n",
        "        spline1 = CubicSpline(x1, y1)\n",
        "\n",
        "        x_sample = np.linspace(x_min_thresh, x_max_thresh, num_points)\n",
        "        y_sample = absolute_difference(x_sample)\n",
        "        area = integrate.simpson(y_sample, x_sample)\n",
        "        diff_cal.append(area)\n",
        "\n",
        "    cal_metric = np.mean(diff_cal)\n",
        "    # print(f\"Calibration metric: {cal_metric:.2f}\")\n",
        "    return cal_metric\n",
        "\n",
        "\n",
        "## small functions\n",
        "def col_gap(col_train, col_test, x_with_constant):\n",
        "    if len(col_train) != len(col_test):\n",
        "        col_gap = [i not in col_test for i in col_train]\n",
        "        x_with_constant[col_train[col_gap]] = 0\n",
        "        x_with_constant = x_with_constant.loc[:, col_train]\n",
        "\n",
        "    return x_with_constant\n",
        "\n",
        "\n",
        "def generate_subsets(input_list):\n",
        "    subsets = []\n",
        "    n = len(input_list)\n",
        "    for subset_size in range(n + 1):\n",
        "        for subset in itertools.combinations(input_list, subset_size):\n",
        "            subsets.append(list(subset))\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHDw9mDnovxk"
      },
      "source": [
        "##fairness_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE_0l1VCRnK7"
      },
      "outputs": [],
      "source": [
        "seed = 1234\n",
        "np.random.seed(seed)\n",
        "rng = np.random.RandomState(seed)\n",
        "\n",
        "\n",
        "class FairBase:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dat_train,\n",
        "        selected_vars,\n",
        "        selected_vars_cat,\n",
        "        y_name,\n",
        "        sen_name,\n",
        "        sen_var_ref,\n",
        "        without_sen=False,\n",
        "        weighted=True,\n",
        "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
        "        class_weight=\"balanced\",\n",
        "    ):\n",
        "        \"\"\"Initialize the fairness base class\n",
        "\n",
        "        Args:\n",
        "            dat_train (data frame): training data\n",
        "            selected_vars (list): selected variables including sensitive variables\n",
        "            selected_vars_cat (list): selected categorical variables\n",
        "            y_name (str): the name of the label\n",
        "            sen_name (list): the name of the sensitive variable\n",
        "            sen_var_ref (dict): the reference level of the sensitive variables\n",
        "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
        "            weighted (bool, optional): compute the weighted version of metrics \"tnr\" and \"tpr\". Defaults to True.\n",
        "            weights (dict, optional): the weightage for \"tnr\" and \"tpr\", summing up to 1. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
        "        \"\"\"\n",
        "\n",
        "        self.dat_train = dat_train\n",
        "        self.vars = selected_vars\n",
        "        self.vars_cat = selected_vars_cat\n",
        "        self.y_name = y_name\n",
        "\n",
        "        if not isinstance(sen_name, list):\n",
        "            self.sen_name = [self.sen_name]\n",
        "        for s in sen_name:\n",
        "            if sen_var_ref[s] not in dat_train[s].unique():\n",
        "                raise ValueError(\n",
        "                    f\"Please provide the right reference level of sensitive variables {s}!\"\n",
        "                )\n",
        "            if s not in self.vars:\n",
        "                self.vars.append(s)\n",
        "        else:\n",
        "            self.sen_name = sen_name\n",
        "            self.sen_var_ref = sen_var_ref\n",
        "\n",
        "        self.without_sen = without_sen\n",
        "        self.weighted = weighted\n",
        "        self.weights = weights\n",
        "        self.class_weight = class_weight\n",
        "\n",
        "    def compute_class_weights(self, y):\n",
        "        \"\"\"Compute class weights for balanced training\n",
        "\n",
        "        Args:\n",
        "            y: target labels\n",
        "\n",
        "        Returns:\n",
        "            array of weights for each sample\n",
        "        \"\"\"\n",
        "        if self.class_weight == \"balanced\":\n",
        "            classes, counts = np.unique(y, return_counts=True)\n",
        "            n_samples = len(y)\n",
        "            n_classes = len(classes)\n",
        "            class_weights = n_samples / (n_classes * counts)\n",
        "            weights = np.array([class_weights[c] for c in y])\n",
        "            return weights\n",
        "        else:\n",
        "            return np.ones(len(y))\n",
        "\n",
        "    def data_process(\n",
        "        self, dat, selected_vars=None, selected_vars_cat=None, without_sen=None\n",
        "    ):\n",
        "        \"\"\"Data preprocess\n",
        "\n",
        "        Args:\n",
        "            dat (data frame): data\n",
        "            selected_vars (list, optional): selected variables (can include sensitive variables). This needs to be provided if the case considered is beyond completely inclusion or exclusion of sensitive variables. Defaults to None.\n",
        "            selected_vars_cat (list, optional): selected categorical variables, subset of selected variables. Defaults to None.\n",
        "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            x_1: predictors with one-coding and with constant\n",
        "            sen_var: the vector of sensitive variable. combined by \"_\", if there are several sensitive variables\n",
        "            y: the vector of the label\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        def combine_sen(dat, sen):\n",
        "            new_sen = [\"_\".join(v) for v in zip(*[dat[s].astype(\"str\") for s in sen])]\n",
        "            return new_sen\n",
        "\n",
        "        if selected_vars is None:\n",
        "            selected_vars = self.vars\n",
        "            selected_vars_cat = self.vars_cat\n",
        "\n",
        "        x = dat.drop(\n",
        "            columns=[\n",
        "                c for c in dat.columns if c == self.y_name or c not in selected_vars\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if (\n",
        "            self.without_sen != \"auto\"\n",
        "            and (without_sen is None and self.without_sen)\n",
        "            or without_sen\n",
        "        ):\n",
        "            x = x.drop(columns=self.sen_name)\n",
        "\n",
        "        if len(self.sen_name) > 1:\n",
        "            sen_var = combine_sen(dat, self.sen_name)\n",
        "        else:\n",
        "            sen_var = dat[self.sen_name[0]]\n",
        "\n",
        "        y = dat[self.y_name]\n",
        "\n",
        "        x_dm, x_groups = _util.model_matrix(x=x, x_names_cat=selected_vars_cat)\n",
        "        x_1 = sm.add_constant(x_dm).astype(\"float\")\n",
        "        return x_1, sen_var, y\n",
        "\n",
        "    def data_prepare(self, dat_expl=None):\n",
        "        \"\"\"Shape the data to AIF360 format\n",
        "\n",
        "        Args:\n",
        "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
        "\n",
        "        \"\"\"\n",
        "        if self.method == \"Unawareness\":\n",
        "            x_with_constant, _, y_train = self.data_process(\n",
        "                self.dat_train if dat_expl is None else dat_expl, without_sen=True\n",
        "            )\n",
        "            return x_with_constant\n",
        "\n",
        "        if self.method_type == \"pre\":\n",
        "            x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
        "            x_with_constant_sen_bin = copy.deepcopy(x_with_constant)\n",
        "            for s in self.sen_name:\n",
        "                x_with_constant_sen_bin[s] = [\n",
        "                    0 if i == self.sen_var_ref[s] else 1 for i in self.dat_train[s]\n",
        "                ]\n",
        "            # print(x_with_constant_expl_sen_bin.columns.head(), flush=True)\n",
        "\n",
        "            pre_train_df = pd.concat([x_with_constant_sen_bin, y_train], axis=1)\n",
        "            pre_train = BinaryLabelDataset(\n",
        "                favorable_label=1,\n",
        "                df=pre_train_df,\n",
        "                label_names=[self.y_name],\n",
        "                protected_attribute_names=self.sen_name,\n",
        "            )\n",
        "            return pre_train\n",
        "\n",
        "        elif self.method_type == \"post\":\n",
        "            if dat_expl is None:\n",
        "                raise ValueError(\"Please provide validation data.\")\n",
        "            else:\n",
        "                x_with_constant_expl, sen_var, y_expl = self.data_process(dat_expl)\n",
        "                x_with_constant_expl_sen_bin = copy.deepcopy(x_with_constant_expl)\n",
        "\n",
        "                for s in self.sen_name:\n",
        "                    x_with_constant_expl_sen_bin[s] = [\n",
        "                        0 if i == self.sen_var_ref[s] else 1 for i in dat_expl[s]\n",
        "                    ]\n",
        "\n",
        "                prob_expl_ori = self.lr_results.predict(x_with_constant_expl)\n",
        "                ori_thresh = find_optimal_cutoff(y_expl, prob_expl_ori)[0]\n",
        "                pred_expl_ori = prob_expl_ori > ori_thresh\n",
        "\n",
        "                post_expl_df = pd.concat([x_with_constant_expl_sen_bin, y_expl], axis=1)\n",
        "                post_expl = BinaryLabelDataset(\n",
        "                    favorable_label=1,\n",
        "                    df=post_expl_df,\n",
        "                    label_names=[self.y_name],\n",
        "                    protected_attribute_names=self.sen_name,\n",
        "                )\n",
        "                post_expl_pred = post_expl.copy(deepcopy=True)\n",
        "                post_expl_pred.scores = prob_expl_ori.values.reshape(-1, 1)\n",
        "                post_expl_pred.labels = pred_expl_ori.values.reshape(-1, 1)\n",
        "                return post_expl, post_expl_pred\n",
        "\n",
        "    def model(self, method_type=None, method=None, dat_expl=None, **kwargs):\n",
        "        \"\"\"Fit the model\n",
        "\n",
        "        Args:\n",
        "            method_type (str, optional): the type of bias mitigation method (pre/in/post). Defaults to None.\n",
        "            method (str, optional): the name of bias mitigation method. Defaults to None.\n",
        "            dat_expl (_type_, optional): validation data needed for post-processing methods. Defaults to None.\n",
        "\n",
        "            Methods:\n",
        "            +------------------+--------------------------------+\n",
        "            | Method type      | Specific methods               |\n",
        "            +==================+================================+\n",
        "            | None             | \"OriginalLR\", \"Unawareness\"   |\n",
        "            +------------------+--------------------------------+\n",
        "            | \"pre\"            | \"Reweigh\"                      |\n",
        "            +------------------+--------------------------------+\n",
        "            | \"in\"             | \"Reductions\"                   |\n",
        "            +------------------+--------------------------------+\n",
        "            | \"post\"           | \"EqOdds\", \"CalEqOdds\", \"ROC\"  |\n",
        "            +------------------+--------------------------------+\n",
        "\n",
        "        Returns:\n",
        "            model results that can be used for prediction\n",
        "        \"\"\"\n",
        "        self.method = method\n",
        "        self.method_type = method_type\n",
        "\n",
        "        if isinstance(self.sen_name, list):\n",
        "            privileged_groups = [{s: 0 for s in self.sen_name}]\n",
        "            unprivileged_groups = [{s: 1 for s in self.sen_name}]\n",
        "        else:\n",
        "            privileged_groups = [{self.sen_name: 0}]\n",
        "            unprivileged_groups = [{self.sen_name: 1}]\n",
        "\n",
        "        x_with_constant_nosen, _, y_train = self.data_process(\n",
        "            self.dat_train, without_sen=True\n",
        "        )\n",
        "        x_with_constant, _, y_train = self.data_process(self.dat_train)\n",
        "\n",
        "        # original LR\n",
        "        if self.class_weight == \"balanced\":\n",
        "            sample_weights = self.compute_class_weights(self.dat_train[self.y_name])\n",
        "            lr_model = sm.GLM(\n",
        "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial(),\n",
        "                freq_weights=sample_weights\n",
        "            )\n",
        "        else:\n",
        "            lr_model = sm.GLM(\n",
        "                self.dat_train[self.y_name], x_with_constant, family=sm.families.Binomial()\n",
        "            )\n",
        "        self.lr_results = lr_model.fit()\n",
        "\n",
        "        if method_type == None:\n",
        "            if method == \"OriginalLR\":\n",
        "                return self.lr_results\n",
        "\n",
        "            elif method == \"Unawareness\":\n",
        "                un_model = sm.GLM(\n",
        "                    self.dat_train[self.y_name],\n",
        "                    x_with_constant_nosen,\n",
        "                    family=sm.families.Binomial(),\n",
        "                )\n",
        "                un_results = un_model.fit()\n",
        "                return un_results\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Please confirm the method: 'OriginalLR' if no bias mitigation is needed; 'Unawareness' if simply excluding the sensitive variabl is enough.\"\n",
        "                )\n",
        "\n",
        "        elif method_type == \"pre\":\n",
        "            pre_train = self.data_prepare()\n",
        "\n",
        "            if method == \"Reweigh\":\n",
        "                reweigh_model = Reweighing(\n",
        "                    privileged_groups=privileged_groups,\n",
        "                    unprivileged_groups=unprivileged_groups,\n",
        "                )\n",
        "                rw_train = reweigh_model.fit_transform(pre_train)\n",
        "                rw_model = sm.GLM(\n",
        "                    self.dat_train[self.y_name],\n",
        "                    x_with_constant,\n",
        "                    family=sm.families.Binomial(),\n",
        "                    freq_weights=rw_train.instance_weights,\n",
        "                )\n",
        "                plt.hist(rw_train.instance_weights)\n",
        "                rw_results = rw_model.fit()\n",
        "                return rw_model, rw_results, rw_train.instance_weights\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Please specify the type of pre-process bias mitigation method among ['Reweigh']!\"\n",
        "                )\n",
        "\n",
        "        elif method_type == \"in\":\n",
        "            if method == \"Reductions\":\n",
        "\n",
        "                constraint = EqualizedOdds(difference_bound=0.01)\n",
        "                np.random.seed(\n",
        "                    0\n",
        "                )  # set seed for consistent results with ExponentiatedGradient\n",
        "                lr_model_sk = LogisticRegression(max_iter=5000, penalty=None)\n",
        "                mitigator = ExponentiatedGradient(lr_model_sk, constraint)\n",
        "\n",
        "                mitigator.fit(\n",
        "                    x_with_constant,\n",
        "                    y_train,\n",
        "                    sensitive_features=self.dat_train[self.sen_name],\n",
        "                )\n",
        "                return mitigator\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Please specify the type of in-process bias mitigation method among ['Reductions']!\"\n",
        "                )\n",
        "\n",
        "        elif method_type == \"post\":\n",
        "            post_expl, post_expl_pred = self.data_prepare(dat_expl=dat_expl)\n",
        "\n",
        "            if method == \"EqOdds\":\n",
        "                eq_model = EqOddsPostprocessing(\n",
        "                    privileged_groups=privileged_groups,\n",
        "                    unprivileged_groups=unprivileged_groups,\n",
        "                    seed=seed,\n",
        "                )\n",
        "                eq_results = eq_model.fit(post_expl, post_expl_pred)\n",
        "                return eq_results\n",
        "\n",
        "            elif method == \"CalEqOdds\":\n",
        "                if \"cost_constraint\" in kwargs:\n",
        "                    cost_constraint = kwargs[\"cost_constraint\"]\n",
        "                else:\n",
        "                    cost_constraint = \"weighted\"  # \"fnr\", \"fpr\", \"weighted\"\n",
        "                cal_eq_model = CalibratedEqOddsPostprocessing(\n",
        "                    privileged_groups=privileged_groups,\n",
        "                    unprivileged_groups=unprivileged_groups,\n",
        "                    cost_constraint=cost_constraint,\n",
        "                    seed=seed,\n",
        "                )\n",
        "                cal_eq_results = cal_eq_model.fit(post_expl, post_expl_pred)\n",
        "                return cal_eq_results\n",
        "\n",
        "            elif method == \"ROC\":\n",
        "                ub = 0.05 if \"ub\" not in kwargs else kwargs[\"ub\"]\n",
        "                lb = -0.05 if \"lb\" not in kwargs else kwargs[\"lb\"]\n",
        "                metric_name = (\n",
        "                    \"Equal opportunity difference\"\n",
        "                    if \"metric_name\" not in kwargs\n",
        "                    else kwargs[\"metric_name\"]\n",
        "                )\n",
        "                # allowed_metrics = [\"Statistical parity difference\", \"Average odds difference\", \"Equal opportunity difference\"]\n",
        "                ROC_model = RejectOptionClassification(\n",
        "                    privileged_groups=privileged_groups,\n",
        "                    unprivileged_groups=unprivileged_groups,\n",
        "                    low_class_thresh=0.01,\n",
        "                    high_class_thresh=0.99,\n",
        "                    num_class_thresh=100,\n",
        "                    num_ROC_margin=50,\n",
        "                    metric_name=metric_name,\n",
        "                    metric_ub=ub,\n",
        "                    metric_lb=lb,\n",
        "                )\n",
        "                ROC_results = ROC_model.fit(post_expl, post_expl_pred)\n",
        "                return ROC_results\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    \"Please specify the type of post-process bias mitigation method among ['EqOdds', 'CalEqOdds', 'ROC']!\"\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Please specify the type of bias mitigation method (pre/in/post)!\"\n",
        "            )\n",
        "\n",
        "    def test(self, dat_test, model=None, params=None, thresh=None, **kwargs):\n",
        "        \"\"\"Test the fairness of the model\n",
        "\n",
        "        Args:\n",
        "            dat_test (data frame): test data\n",
        "            model (_type_, optional): the fitted model. Defaults to None.\n",
        "            params (_type_, optional): coefficients for the model. Defaults to None.\n",
        "            thresh (_type_, optional): threshold for the predictions. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            pred_test / prob_test (array): predicted labels / predicted probabilities\n",
        "            fairmetrics (data frame): fairness metrics\n",
        "            fairsummary (data frame): fairness summary for each subgroup\n",
        "        \"\"\"\n",
        "        if \"without_sen\" in kwargs.keys():\n",
        "            without_sen = kwargs[\"without_sen\"]\n",
        "        else:\n",
        "            without_sen = self.without_sen\n",
        "        x_with_constant_test, sen_var, y_test = self.data_process(dat_test)\n",
        "        prob_test = None\n",
        "        thresh = None\n",
        "\n",
        "        if model is not None:\n",
        "            if self.method_type == \"post\":\n",
        "                _, post_test_pred = self.data_prepare(dat_expl=dat_test)\n",
        "                pred_test = model.predict(post_test_pred).labels.reshape(-1)\n",
        "            else:\n",
        "                if self.method_type == None and self.method == \"Unawareness\":\n",
        "                    x_with_constant_test = self.data_prepare(dat_expl=dat_test)\n",
        "\n",
        "                if self.method == \"Reductions\":\n",
        "                    pred_test = model.predict(x_with_constant_test)\n",
        "                else:\n",
        "                    prob_test = model.predict(x_with_constant_test)\n",
        "                    thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
        "                    pred_test = prob_test > thresh\n",
        "                    # print(prob_test)\n",
        "        else:\n",
        "            raise ValueError(\"Please provide the right model!\")\n",
        "\n",
        "        fe = FAIMEvaluator(\n",
        "            y_true=y_test,\n",
        "            y_pred=prob_test,\n",
        "            y_pred_bin=pred_test,\n",
        "            sen_var=sen_var,\n",
        "            weighted=self.weighted,\n",
        "            weights=self.weights,\n",
        "        )\n",
        "        fairmetrics = fe.fairmetrics\n",
        "        fairsummary = fe.fairsummary\n",
        "        clametrics = fe.clametrics\n",
        "\n",
        "        return pred_test if prob_test is None else prob_test, fairmetrics, clametrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmuQmuFto1c1"
      },
      "source": [
        "##fairness_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt0L8vOiyVsC"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "my_fairness_bases = {\n",
        "    \"tpr\": recall_score,\n",
        "    \"tnr\": true_negative_rate,\n",
        "    \"sr\": selection_rate,\n",
        "    \"acc\": accuracy_score,\n",
        "    \"conf_mat\": confusion_matrix,\n",
        "}\n",
        "# the situation for each group should not be bad; tnr -> fpr\n",
        "my_bases_bound = {\"tpr\": 0.6, \"tnr\": 0.6, \"sr\": 0, \"acc\": 0.6, \"conf_mat\": pd.NA}\n",
        "\n",
        "\n",
        "def fairarea(fairness_metrics):\n",
        "    n_metric = len(fairness_metrics)\n",
        "    tmp = fairness_metrics.values.flatten().tolist()\n",
        "    tmp_1 = tmp[1:] + tmp[:1]\n",
        "\n",
        "    if n_metric > 2:\n",
        "        theta_c = 2 * np.pi / n_metric\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
        "    elif n_metric == 2:\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
        "    else:\n",
        "        area = np.abs(tmp[0])\n",
        "\n",
        "    return area\n",
        "\n",
        "\n",
        "class FAIMEvaluator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        y_pred_bin,\n",
        "        sen_var,\n",
        "        fair_only=False,\n",
        "        cla_metrics=[\"auc\"],\n",
        "        weighted=False,\n",
        "        weights=None,\n",
        "        bases=my_fairness_bases,\n",
        "        bound=my_bases_bound,\n",
        "    ):\n",
        "        \"\"\"Initialize the fairness evaluator.\n",
        "\n",
        "        Args:\n",
        "            y_true: true labels\n",
        "            y_pred: predicted scores or probabilities\n",
        "            y_pred_bin: predicted binary labels\n",
        "            sen_var: the vector of sensitive variables\n",
        "            fair_only (bool, optional): whether to compute fairness metrics only. Defaults to False.\n",
        "            cla_metrics (list, optional): classification metrics. Defaults to [\"auc\"].\n",
        "            weighted (bool, optional): whether to create a customized fairness metric based on weighted combining of 'tnr' and 'tpr'. Defaults to False.\n",
        "            weights (_type_, optional): the weights for weighted combining of 'tnr' and 'tpr'. Required when `weighted` is True. Defaults to None.\n",
        "            bases (_type_, optional): the bases for fairness metrics. Defaults to my_fairness_bases (see above).\n",
        "            bound (_type_, optional): the bound for base metrics. Defaults to my_bases_bound.\n",
        "        \"\"\"\n",
        "        # super().__init__(y_obs=y_true, y_pred=y_pred, y_pred_bin=y_pred_bin, sens_var=pd.Series(sen_var), y_pos=True)\n",
        "\n",
        "        self.y_true = y_true\n",
        "        self.y_pred = y_pred\n",
        "        self.y_pred_bin = pd.Series(y_pred_bin)\n",
        "        self.sen_var = pd.Series(sen_var)\n",
        "        self.cla_metrics = cla_metrics\n",
        "\n",
        "        self.my_fairness_bases = bases\n",
        "        self.my_bases_bound = bound\n",
        "\n",
        "        if weighted:\n",
        "            if weights is None or not isinstance(weights, dict):\n",
        "                raise TypeError(\n",
        "                    \"The weights need to be specified and the type should be dict!\"\n",
        "                )\n",
        "            elif len(weights) != 2 or np.sum(list(weights.values())) != 1:\n",
        "                raise ValueError(\n",
        "                    \"The weights should be a dict containing two values respectively for 'tpr' and 'tnr'. In addition, the sum of weights should be equal to 1!\"\n",
        "                )\n",
        "            else:\n",
        "                self.weighted = weighted\n",
        "                self.weights = weights\n",
        "\n",
        "        self._fairsummary_generation()\n",
        "        self._fairmetrics_generation()\n",
        "        if not fair_only:\n",
        "            if cla_metrics is None:\n",
        "                raise ValueError(\"The classification metrics should be specified!\")\n",
        "            self._clametric_generation()\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_sen(y_obs, sen_var, sens_var_ref):\n",
        "        # print(\"Checking the sensitive variable...\")\n",
        "        return {\"sens_var\": sen_var, \"sens_var_ref\": pd.unique(sen_var)[0]}\n",
        "\n",
        "    def _fairsummary_generation(self):\n",
        "        \"\"\"Computation primary metrics (e.g., TPR, TNR, etc.) among subgroups\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "        self.fairsummary = MetricFrame(\n",
        "            y_true=self.y_true,\n",
        "            y_pred=self.y_pred_bin,\n",
        "            metrics=self.my_fairness_bases,\n",
        "            sensitive_features=self.sen_var,\n",
        "        )\n",
        "\n",
        "        # Create performance metrics table with additional metrics\n",
        "        by_group = self.fairsummary.by_group\n",
        "\n",
        "        tpr = by_group[\"tpr\"]\n",
        "        tnr = by_group[\"tnr\"]\n",
        "        sr = by_group[\"sr\"]\n",
        "        acc = by_group[\"acc\"]\n",
        "\n",
        "        # Calculate FPR, FNR, and BER\n",
        "        fpr = 1 - tnr\n",
        "        fnr = 1 - tpr\n",
        "        ber = (fpr + fnr) / 2\n",
        "\n",
        "        # Create comprehensive performance table\n",
        "        self.performance_table = pd.DataFrame({\n",
        "            \"TPR\": tpr,\n",
        "            \"FPR\": fpr,\n",
        "            \"TNR\": tnr,\n",
        "            \"FNR\": fnr,\n",
        "            \"SR\": sr,\n",
        "            \"Accuracy\": acc,\n",
        "            \"BER\": ber\n",
        "        })\n",
        "\n",
        "    def _fairmetrics_generation(self):\n",
        "        \"\"\"Generate fairness metrics and disparity tables.\"\"\"\n",
        "\n",
        "        # ----- machine learning performance-based fairness metrics -----\n",
        "        bases = self.my_fairness_bases.keys()\n",
        "        fairmetrics = {}\n",
        "        qc = {}\n",
        "        diff_ = self.fairsummary.difference()\n",
        "        for b in list(bases)[:-1]:\n",
        "            qc[b] = self.fairsummary.overall[b] > self.my_bases_bound[b]\n",
        "\n",
        "        fairmetrics[\"Equal Opportunity\"] = diff_[\"tpr\"]\n",
        "        fairmetrics[\"Equalized Odds\"] = np.max([diff_[\"tpr\"], diff_[\"tnr\"]])\n",
        "        fairmetrics[\"Statistical Parity\"] = diff_[\"sr\"]\n",
        "        fairmetrics[\"Accuracy Equality\"] = diff_[\"acc\"]\n",
        "\n",
        "        # MODIFIED: Calculate true BER Equality as Range(BER)\n",
        "        by_group = self.fairsummary.by_group\n",
        "        fpr_values = 1 - by_group[\"tnr\"]\n",
        "        fnr_values = 1 - by_group[\"tpr\"]\n",
        "        ber_values = 0.5 * (fpr_values + fnr_values)\n",
        "\n",
        "        if self.weighted:\n",
        "            # True BER Equality = Range of BER across groups\n",
        "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
        "        else:\n",
        "            fairmetrics[\"BER Equality\"] = ber_values.max() - ber_values.min()\n",
        "\n",
        "        self.fairmetrics = pd.DataFrame([fairmetrics])\n",
        "\n",
        "        # Create fairness disparity summary table\n",
        "        diff_ = self.fairsummary.difference()\n",
        "\n",
        "        # Get TPR, FPR disparities\n",
        "        tpr_diff = diff_[\"tpr\"]\n",
        "        fpr_diff = abs(diff_[\"tnr\"])  # FPR diff = TNR diff\n",
        "\n",
        "        # Equalized Odds = max of TPR and FPR differences\n",
        "        equalized_odds = max(tpr_diff, fpr_diff)\n",
        "\n",
        "        # Equal Opportunity = TPR difference\n",
        "        equal_opportunity = tpr_diff\n",
        "\n",
        "        # BER Equality (now correctly calculated)\n",
        "        ber_equality = fairmetrics[\"BER Equality\"]\n",
        "\n",
        "        # Helper function to determine if we're looking at intersectional groups\n",
        "        def is_intersectional(group_name):\n",
        "            \"\"\"Check if group contains multiple attributes (e.g., 'Male_White')\"\"\"\n",
        "            return '_' in str(group_name)\n",
        "\n",
        "        # MODIFIED: Helper function to check if reference group exists\n",
        "        def get_reference_group(groups):\n",
        "            \"\"\"Find reference group based on sensitive variable type\"\"\"\n",
        "            # For intersectional (Sex_Race)\n",
        "            for group in groups:\n",
        "                if '@Male_@White' in str(group):\n",
        "                    return group\n",
        "\n",
        "            # For race only\n",
        "            for group in groups:\n",
        "                if '@White' in str(group) and '@Male' not in str(group):\n",
        "                    return group\n",
        "\n",
        "            # For sex only\n",
        "            for group in groups:\n",
        "                if '@Male' in str(group) and '@White' not in str(group):\n",
        "                    return group\n",
        "\n",
        "            return None\n",
        "\n",
        "        # Determine min type for Equalized Odds (which metric drives the disparity)\n",
        "        if tpr_diff >= fpr_diff:\n",
        "            eq_odds_min_type = \"TPR\"\n",
        "            eq_odds_values = by_group[\"tpr\"]\n",
        "        else:\n",
        "            eq_odds_min_type = \"FPR\"\n",
        "            eq_odds_values = 1 - by_group[\"tnr\"]\n",
        "\n",
        "        # Build disparity table with 3 rows\n",
        "        disparity_data = []\n",
        "\n",
        "        # Determine the appropriate column name based on group structure\n",
        "        groups = by_group.index.tolist()\n",
        "        is_intersect = is_intersectional(groups[0]) if len(groups) > 0 else False\n",
        "        group_col_name = \"Intersection\" if is_intersect else \"Group\"\n",
        "\n",
        "        # Check if reference group exists\n",
        "        reference_group = get_reference_group(groups)\n",
        "\n",
        "        # Row 1: Equalized Odds (max of TPR/FPR)\n",
        "        row1 = {\n",
        "            \"Metric\": \"Equalized Odds (max of TPR/FPR)\",\n",
        "            \"Min Value\": eq_odds_values.min(),\n",
        "            f\"Min {group_col_name}\": eq_odds_values.idxmin(),\n",
        "            \"Min Type\": eq_odds_min_type,\n",
        "            \"Max Value\": eq_odds_values.max(),\n",
        "            f\"Max {group_col_name}\": eq_odds_values.idxmax(),\n",
        "            \"Gap\": equalized_odds\n",
        "        }\n",
        "\n",
        "        # Add reference group if it exists and is not already min/max\n",
        "        if reference_group and reference_group not in [eq_odds_values.idxmin(), eq_odds_values.idxmax()]:\n",
        "            row1[f\"Reference {group_col_name}\"] = reference_group\n",
        "            row1[\"Reference Value\"] = eq_odds_values[reference_group]\n",
        "            row1[\"Reference Gap\"] = abs(eq_odds_values[reference_group] - eq_odds_values.min())\n",
        "        else:\n",
        "            row1[f\"Reference {group_col_name}\"] = None\n",
        "            row1[\"Reference Value\"] = None\n",
        "            row1[\"Reference Gap\"] = None\n",
        "\n",
        "        disparity_data.append(row1)\n",
        "\n",
        "        # Row 2: Equal Opportunity (TPR)\n",
        "        tpr_values = by_group[\"tpr\"]\n",
        "        row2 = {\n",
        "            \"Metric\": \"Equal Opportunity (TPR)\",\n",
        "            \"Min Value\": tpr_values.min(),\n",
        "            f\"Min {group_col_name}\": tpr_values.idxmin(),\n",
        "            \"Min Type\": \"TPR\",\n",
        "            \"Max Value\": tpr_values.max(),\n",
        "            f\"Max {group_col_name}\": tpr_values.idxmax(),\n",
        "            \"Gap\": equal_opportunity\n",
        "        }\n",
        "\n",
        "        # Add reference group if it exists and is not already min/max\n",
        "        if reference_group and reference_group not in [tpr_values.idxmin(), tpr_values.idxmax()]:\n",
        "            row2[f\"Reference {group_col_name}\"] = reference_group\n",
        "            row2[\"Reference Value\"] = tpr_values[reference_group]\n",
        "            row2[\"Reference Gap\"] = abs(tpr_values[reference_group] - tpr_values.min())\n",
        "        else:\n",
        "            row2[f\"Reference {group_col_name}\"] = None\n",
        "            row2[\"Reference Value\"] = None\n",
        "            row2[\"Reference Gap\"] = None\n",
        "\n",
        "        disparity_data.append(row2)\n",
        "\n",
        "        # Row 3: BER Equality (CORRECTED)\n",
        "        row3 = {\n",
        "            \"Metric\": \"BER Equality\",\n",
        "            \"Min Value\": ber_values.min(),\n",
        "            f\"Min {group_col_name}\": ber_values.idxmin(),\n",
        "            \"Min Type\": \"BER\",\n",
        "            \"Max Value\": ber_values.max(),\n",
        "            f\"Max {group_col_name}\": ber_values.idxmax(),\n",
        "            \"Gap\": ber_equality\n",
        "        }\n",
        "\n",
        "        # Add reference group if it exists and is not already min/max\n",
        "        if reference_group and reference_group not in [ber_values.idxmin(), ber_values.idxmax()]:\n",
        "            row3[f\"Reference {group_col_name}\"] = reference_group\n",
        "            row3[\"Reference Value\"] = ber_values[reference_group]\n",
        "            row3[\"Reference Gap\"] = abs(ber_values[reference_group] - ber_values.min())\n",
        "        else:\n",
        "            row3[f\"Reference {group_col_name}\"] = None\n",
        "            row3[\"Reference Value\"] = None\n",
        "            row3[\"Reference Gap\"] = None\n",
        "\n",
        "        disparity_data.append(row3)\n",
        "\n",
        "        self.disparity_table = pd.DataFrame(disparity_data)\n",
        "\n",
        "        # Round numeric columns to 4 decimals\n",
        "        numeric_cols = [\"Min Value\", \"Max Value\", \"Gap\"]\n",
        "        if \"Reference Value\" in self.disparity_table.columns:\n",
        "            numeric_cols.append(\"Reference Value\")\n",
        "        if \"Reference Gap\" in self.disparity_table.columns:\n",
        "            numeric_cols.append(\"Reference Gap\")\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in self.disparity_table.columns:\n",
        "                self.disparity_table[col] = self.disparity_table[col].round(4)\n",
        "\n",
        "        self.qc = pd.DataFrame([qc])\n",
        "\n",
        "    def _clametric_generation(self):\n",
        "        clametrics = {}\n",
        "        pred = self.y_pred if self.y_pred is not None else self.y_pred_bin\n",
        "\n",
        "        # Add sensitivity (TPR) and specificity (TNR)\n",
        "        clametrics[\"sensitivity\"] = recall_score(self.y_true, self.y_pred_bin)\n",
        "        clametrics[\"specificity\"] = true_negative_rate(self.y_true, self.y_pred_bin)\n",
        "\n",
        "        if \"auc\" in self.cla_metrics:\n",
        "            # clametrics[\"auc\"] = roc_auc_score(self.y_true, self.y_pred)\n",
        "            clametrics[\"auc_low\"], clametrics[\"auc\"], clametrics[\"auc_high\"] = (\n",
        "                get_ci_auc(self.y_true, pred, alpha=0.05, type=\"auc\")\n",
        "            )\n",
        "\n",
        "        self.clametrics = pd.DataFrame([clametrics])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJcE3tUlpA6m"
      },
      "source": [
        "##fairness_modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djWDrJNBRvnn"
      },
      "outputs": [],
      "source": [
        "class FAIMGenerator(FairBase):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dat_train,\n",
        "        selected_vars,\n",
        "        selected_vars_cat,\n",
        "        y_name,\n",
        "        sen_name,\n",
        "        sen_var_ref,\n",
        "        output_dir,\n",
        "        criterion=\"loss\",\n",
        "        epsilon=0.05,\n",
        "        m=800,\n",
        "        n_final=350,\n",
        "        without_sen=False,\n",
        "        pre=False,\n",
        "        pre_method=\"Reweigh\",\n",
        "        post=False,\n",
        "        post_method=\"equalizedodds\",\n",
        "        class_weight=\"balanced\",\n",
        "    ):\n",
        "        \"\"\"Initialize the class of FAIM\n",
        "\n",
        "        Args:\n",
        "            dat_train (data frame): the training data\n",
        "            selected_vars (list): the selected variables that include sensitive variables\n",
        "            selected_vars_cat (list): the selected categorical variables that include sensitive variables\n",
        "            y_name (str): the name of the label, e.g. \"y\", \"label\", etc.\n",
        "            sen_name (list): the name of the sensitive variables\n",
        "            sen_var_ref (dict): the reference values of the sensitive variables e.g. {\"gender\": \"F\"}\n",
        "            output_dir: the output directory to store the nearly optimal models results\n",
        "            criterion (str, optional): the criterion to generate nearly optimal models. Defaults to \"loss\".\n",
        "            epsilon (float, optional): the control factor of \"nearly optimality\", i.e. the gap to the optimal model. Defaults to 0.05.\n",
        "            without_sen (bool, optional): directly exclude the sensitive variables. Defaults to False.\n",
        "            pre (bool, optional): whether to use pre-process bias mitigation methods before FAIM. Defaults to False.\n",
        "            pre_method (str, optional): specific pre-process method. Defaults to \"Reweigh\".\n",
        "            post (bool, optional): whether to use post-process bias mitigation methods after FAIM. Defaults to False.\n",
        "            post_method (str, optional): specific post-process method. Defaults to \"EqOdds\".\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(\n",
        "            dat_train,\n",
        "            selected_vars,\n",
        "            selected_vars_cat,\n",
        "            y_name,\n",
        "            sen_name,\n",
        "            sen_var_ref,\n",
        "            without_sen,\n",
        "            class_weight=class_weight,\n",
        "        )\n",
        "\n",
        "        self.criterion = criterion\n",
        "        self.output_dir = output_dir\n",
        "        self.epsilon = epsilon\n",
        "        self.m = m\n",
        "        self.n_final = n_final\n",
        "\n",
        "        self.pre = pre\n",
        "        if pre:\n",
        "            self.pre_method = pre_method\n",
        "            self.rw_model, self.rw_results, self.rw_weights = self.pre_mitigate()\n",
        "            plt.hist(self.rw_weights)\n",
        "        if post:\n",
        "            self.post = post\n",
        "            self.post_method = post_method\n",
        "\n",
        "        self.optim_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
        "        self.optim_results = self.optim_obj.model_optim\n",
        "        self.optim_model = self.optim_obj.model_optim.model\n",
        "\n",
        "        self.dat_expl = None\n",
        "        self.dat_test = None\n",
        "\n",
        "    # def __reduce__(self):\n",
        "    #     return (self.__class__, (self.coefs, self.best_coef, self.best_optim_base_obj, self.best_sen_exclusion, self.fairmetrics_df))\n",
        "\n",
        "    def pre_mitigate(self):\n",
        "        \"\"\"Pre-process bias mitigation methods\"\"\"\n",
        "        rw_model, rw_results, instance_weights = self.model(\n",
        "            method_type=\"pre\", method=self.pre_method\n",
        "        )\n",
        "\n",
        "        return rw_model, rw_results, instance_weights\n",
        "\n",
        "    def optimal_model(self, selected_vars, selected_vars_cat):\n",
        "        \"\"\"Generate the optimal model\"\"\"\n",
        "        x = self.dat_train.drop(\n",
        "            columns=[\n",
        "                c\n",
        "                for c in self.dat_train.columns\n",
        "                if c == self.y_name or c not in selected_vars\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # ===== HANDLE THREE CASES =====\n",
        "\n",
        "        # Case 1: Using pre-processing fairness intervention (Reweighing)\n",
        "        if self.pre and self.pre_method == \"Reweigh\":\n",
        "            sample_weights = self.rw_weights  # Use Reweighing weights\n",
        "            print(\" Using Reweighing pre-processing weights (fairness intervention)\")\n",
        "\n",
        "        # Case 2: Using balanced class weights (for imbalance, not fairness)\n",
        "        elif self.class_weight == \"balanced\":\n",
        "            y_train = self.dat_train[self.y_name]\n",
        "            n_samples = len(y_train)\n",
        "            classes, counts = np.unique(y_train, return_counts=True)\n",
        "            class_weight_dict = {\n",
        "                cls: n_samples / (len(classes) * cnt)\n",
        "                for cls, cnt in zip(classes, counts)\n",
        "            }\n",
        "            sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
        "            print(f\" Using balanced class weights (for imbalance): {class_weight_dict}\")\n",
        "\n",
        "        # Case 3: No weighting\n",
        "        else:\n",
        "            sample_weights = None\n",
        "            print(\" No sample weighting\")\n",
        "\n",
        "        model_object = model.models(\n",
        "            x=x,\n",
        "            y=self.dat_train[self.y_name],\n",
        "            x_names_cat=selected_vars_cat,\n",
        "            output_dir=self.output_dir,\n",
        "            criterion=self.criterion,\n",
        "            sample_w=sample_weights\n",
        "        )\n",
        "\n",
        "        return model_object\n",
        "\n",
        "    def nearly_optimal_model(self, optim_base_obj, m=200, n_final=50, epsilon=None):\n",
        "        \"\"\"Generate the nearly optimal models\n",
        "\n",
        "        Args:\n",
        "            optim_base_obj (object): the object of the optimal model\n",
        "            m (int, optional): the number of models to be generated. Defaults to 800.\n",
        "            n_final (int, optional): the number of nearly optimal models to be generated. Defaults to 350.\n",
        "\n",
        "        Returns:\n",
        "            coefs (data frame): the coefficients of the nearly optimal models\n",
        "            plots (plot): the plot of the status nearly optimal models\n",
        "        \"\"\"\n",
        "        if epsilon is None:\n",
        "            epsilon = self.epsilon\n",
        "\n",
        "        u1, u2 = optim_base_obj.init_hyper_params(m=m)\n",
        "        optim_base_obj.draw_models(\n",
        "            u1=u1,\n",
        "            u2=u2,\n",
        "            m=self.m,\n",
        "            n_final=self.n_final,\n",
        "            random_state=1234\n",
        "        )\n",
        "        coefs = pd.read_csv(\n",
        "            os.path.join(self.output_dir, \"models_near_optim.csv\"), index_col=0\n",
        "        )\n",
        "        return coefs, optim_base_obj.models_plot\n",
        "\n",
        "    def fairness_compute(\n",
        "        self,\n",
        "        dat_expl,\n",
        "        optim_base_obj,\n",
        "        coefs,\n",
        "        weighted=True,\n",
        "        weights={\"tnr\": 0.5, \"tpr\": 0.5},\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Compute the fairness metrics of the nearly optimal models\n",
        "\n",
        "        Args:\n",
        "            dat_expl (data frame): the data frame of the validation data\n",
        "            optim_base_obj (object): the object of the optimal model\n",
        "            coefs (data frame): the coefficients of the nearly optimal models\n",
        "            weighted (bool, optional): whether to use weighted fairness metrics. Defaults to True.\n",
        "            weights (dict, optional): the weights of the weighted fairness metrics. Defaults to {\"tnr\": 0.5, \"tpr\": 0.5}.\n",
        "            **kwargs: the other parameters of the fairness computation\n",
        "\n",
        "        Returns:\n",
        "            fairmetrics_df (data frame): the fairness metrics of the nearly optimal models\n",
        "            qc_df (data frame): the quality control results of the nearly optimal models\n",
        "        \"\"\"\n",
        "        if weighted:\n",
        "            self.weighted = weighted\n",
        "            self.weights = weights\n",
        "        self.dat_expl = dat_expl\n",
        "\n",
        "        optim_base_results = optim_base_obj.model_optim\n",
        "        optim_base_model = optim_base_obj.model_optim.model\n",
        "\n",
        "        fairmetrics_df = []\n",
        "        qc_df = []\n",
        "        by_group_list = []\n",
        "\n",
        "        for i in range(coefs.shape[0]):\n",
        "            coef = coefs.drop(columns=[\"perf_metric\"]).iloc[i, :]\n",
        "            x_with_constant, sen_var, y_expl = self.data_process(dat_expl, **kwargs)\n",
        "\n",
        "            # sen_var = dat_expl[self.sen_name]\n",
        "            optim_base_results.params = coef\n",
        "\n",
        "            col_train = optim_base_results.params.index\n",
        "            col_test = x_with_constant.columns\n",
        "            x_with_constant = col_gap(col_train, col_test, x_with_constant)\n",
        "\n",
        "            prob_expl = optim_base_model.predict(params=coef, exog=x_with_constant)\n",
        "            thresh = find_optimal_cutoff(y_expl, prob_expl)[0]\n",
        "            pred_expl = prob_expl > thresh\n",
        "\n",
        "            fe = FAIMEvaluator(\n",
        "                y_true=y_expl,\n",
        "                y_pred=prob_expl,\n",
        "                y_pred_bin=pred_expl,\n",
        "                sen_var=sen_var,\n",
        "                fair_only=True,\n",
        "                weighted=weighted,\n",
        "                weights=weights,\n",
        "            )\n",
        "            fairmetrics = fe.fairmetrics\n",
        "            qc = fe.qc\n",
        "\n",
        "            fairmetrics_df.append(fairmetrics)\n",
        "            qc_df.append(qc)\n",
        "            by_group_list.append(fe.fairsummary)\n",
        "\n",
        "        fairmetrics_df = pd.concat(fairmetrics_df).reset_index(drop=True)\n",
        "        qc_df = pd.concat(qc_df)\n",
        "\n",
        "        return fairmetrics_df, qc_df\n",
        "        # self.thresh_list = thresh_list\n",
        "\n",
        "    def compare(self, dat_expl, optim_base_results, selected_vars, selected_vars_cat):\n",
        "        \"\"\"Compare the cases of exclusion of sensitive variables with the original optimal model i.e. no exclusion of sensitive variables\n",
        "\n",
        "        Args:\n",
        "            dat_expl (data frame): the data frame of the validation data\n",
        "            optim_base_results (object): the object of the optimal model regarding the specific case of exclusion of sensitive variables\n",
        "            selected_vars (list): the selected variables that can include sensitive variables\n",
        "            selected_vars_cat (list): the selected categorical variables that can include sensitive variables\n",
        "\n",
        "        Returns:\n",
        "            bool: whether the case of exclusion of sensitive variables will be expanded to the nearly optimal models\n",
        "\n",
        "        \"\"\"\n",
        "        x_with_constant_ori, sen_var, y_expl = self.data_process(\n",
        "            dat_expl, selected_vars=self.vars, selected_vars_cat=self.vars_cat\n",
        "        )\n",
        "        x_with_constant_base, sen_var, y_expl = self.data_process(\n",
        "            dat_expl, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
        "        )\n",
        "        pred_ori = self.optim_results.predict(x_with_constant_ori)\n",
        "        pred_base = optim_base_results.predict(x_with_constant_base)\n",
        "\n",
        "        if self.criterion == \"auc\":\n",
        "            auc_ori = roc_auc_score(y_expl, pred_ori)\n",
        "            auc_base = roc_auc_score(y_expl, pred_base)\n",
        "\n",
        "            # return auc_base > auc_ori * (1-np.sqrt(self.epsilon))\n",
        "            return auc_base > auc_ori * np.sqrt(1 - self.epsilon)\n",
        "        if self.criterion == \"loss\":\n",
        "            loss_ori = self.optim_model.loglike(self.optim_results.params)\n",
        "            loss_base = optim_base_results.model.loglike(optim_base_results.params)\n",
        "            ratio = loss_base / loss_ori\n",
        "            print(f\"loss_ori: {loss_ori}, loss_base: {loss_base}, ratio: {ratio}\")\n",
        "            return ratio < np.sqrt(1 + self.epsilon)\n",
        "\n",
        "    def FAIM_model(self, dat_expl):\n",
        "        \"\"\"FAIM: Generate the nearly optimal models and compute the fairness metrics of the nearly optimal models\n",
        "\n",
        "        Args:\n",
        "            dat_expl (data frame): the data frame of the validation data\n",
        "        \"\"\"\n",
        "        self.dat_expl = dat_expl\n",
        "\n",
        "        self.coefs = {}\n",
        "        self.plots = {}\n",
        "        self.optim_base_obj_list = {}\n",
        "        self.fairmetrics_df = pd.DataFrame()\n",
        "        self.qc_df = pd.DataFrame()\n",
        "\n",
        "        if self.without_sen == \"auto\":\n",
        "            sen_senarios = generate_subsets(self.sen_name)\n",
        "            pbar = tqdm(\n",
        "                sen_senarios, desc=\"Generating nearly optimal models\", postfix=\"*Start*\"\n",
        "            )\n",
        "            for x in pbar:\n",
        "                pbar.set_postfix(postfix=f\"exclusion: {x}\")\n",
        "\n",
        "                selected_vars = [i for i in self.vars if i not in x]\n",
        "                selected_vars_cat = [i for i in self.vars_cat if i not in x]\n",
        "                optim_base_obj = self.optimal_model(selected_vars, selected_vars_cat)\n",
        "                optim_base_results = optim_base_obj.model_optim\n",
        "\n",
        "                if self.compare(\n",
        "                    dat_expl, optim_base_results, selected_vars, selected_vars_cat\n",
        "                ):\n",
        "                    self.optim_base_obj_list[\"_\".join(x)] = optim_base_obj\n",
        "\n",
        "                    if self.criterion == \"auc\":\n",
        "                        epsilon = 1 - np.sqrt(1 - self.epsilon)\n",
        "                    elif self.criterion == \"loss\":\n",
        "                        epsilon = np.sqrt(1 + self.epsilon) - 1\n",
        "\n",
        "                    coefs, plots = self.nearly_optimal_model(\n",
        "                        optim_base_obj, n_final=self.n_final, epsilon=epsilon\n",
        "                    )\n",
        "                    self.coefs[\"_\".join(x)] = coefs\n",
        "                    self.plots[\"_\".join(x)] = plots\n",
        "                    fairmetrics_df, qc_df = self.fairness_compute(\n",
        "                        dat_expl,\n",
        "                        optim_base_obj,\n",
        "                        coefs,\n",
        "                        selected_vars=selected_vars,\n",
        "                        selected_vars_cat=selected_vars_cat,\n",
        "                    )\n",
        "                    fairmetrics_df[\"auc\"] = coefs[\"perf_metric\"]\n",
        "                    qc_df[\"auc\"] = coefs[\"perf_metric\"]\n",
        "\n",
        "                    fairmetrics_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
        "                    qc_df[\"sen_var_exclusion\"] = \"_\".join(x)\n",
        "                    self.fairmetrics_df = pd.concat(\n",
        "                        [self.fairmetrics_df, fairmetrics_df]\n",
        "                    )\n",
        "                    self.qc_df = pd.concat([self.qc_df, qc_df])\n",
        "\n",
        "                else:\n",
        "                    print(f\"Exclusion of {x} degrades the discrimination performance!\")\n",
        "\n",
        "            self.fairmetrics_df = self.fairmetrics_df.reset_index(drop=True)\n",
        "            self.qc_df = self.qc_df.reset_index(drop=True)\n",
        "\n",
        "            # return fairmetrics_df, qc_df\n",
        "\n",
        "    def describe(self, selected_metrics=None):\n",
        "        \"\"\"Describe the distribution of fairness metrics for all nearly optimal models\n",
        "        Args:\n",
        "            selected_metrics (list, optional): the selected fairness metrics, e.g. [\"Statistical Parity\", \"Equalized Odds\", \"Average Accuracy\"]. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            fig: the plot of the distribution of fairness metrics for all nearly optimal models\n",
        "        \"\"\"\n",
        "        sen_var_exclusion = self.fairmetrics_df[\"sen_var_exclusion\"]\n",
        "        auc_var = self.fairmetrics_df[\"auc\"]\n",
        "        fairmetrics_df = self.fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
        "        qc_df = self.qc_df.drop(columns=[\"sen_var_exclusion\", \"auc\"])\n",
        "\n",
        "        if selected_metrics is None:\n",
        "            num_metrics = fairmetrics_df.shape[1]\n",
        "        else:\n",
        "            for m in selected_metrics:\n",
        "                if m not in fairmetrics_df.columns:\n",
        "                    raise ValueError(f\"The metric {m} is not in the fairness metrics!\")\n",
        "\n",
        "            qc_df = qc_df[selected_metrics]\n",
        "            fairmetrics_df = fairmetrics_df[selected_metrics]\n",
        "            num_metrics = len(selected_metrics)\n",
        "\n",
        "        ids_after_qc = np.arange(qc_df.shape[0])[\n",
        "            (np.sum(qc_df, 1) == qc_df.shape[1]).tolist()\n",
        "        ]\n",
        "        print(f\"{len(ids_after_qc)} are qualified after quality control\")\n",
        "\n",
        "        min_ones = fairmetrics_df.iloc[ids_after_qc, :].apply(axis=0, func=np.argmin)\n",
        "\n",
        "        for m in min_ones.index:\n",
        "            id = fairmetrics_df.iloc[ids_after_qc, :].index[min_ones[m]]\n",
        "            print(\n",
        "                f\"the model with minimal {m}: No.{id} -- {fairmetrics_df.loc[id, m]:.3f}, with {sen_var_exclusion[id]} excluded from regression\"\n",
        "            )\n",
        "\n",
        "        plot_df = self.fairmetrics_df.loc[ids_after_qc, :]\n",
        "        fig = plot_distribution(plot_df)\n",
        "\n",
        "        if len(ids_after_qc) < fairmetrics_df.shape[0]:\n",
        "            return (\n",
        "                fairmetrics_df.iloc[ids_after_qc, :],\n",
        "                [sen_var_exclusion[i] for i in ids_after_qc],\n",
        "                fig,\n",
        "            )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def transmit(\n",
        "        self,\n",
        "        targeted_metrics=[\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"],\n",
        "        thresh_show=0.3,\n",
        "        best_id=None,\n",
        "        best_sen_exclusion=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"Select the best model regarding fairness\n",
        "\n",
        "        Args:\n",
        "            targeted_metrics (list, optional): the targeted fairness metrics. Defaults to [\"Average Accuracy\", \"Statistical Parity\", \"Equalized Odds\"].\n",
        "            thresh_show (float, optional): the threshold to filter the models. Defaults to 0.3.\n",
        "\n",
        "        Returns:\n",
        "            best_coef: the coefficients of the best model\n",
        "            best_sen_exclusion: the sensitive variables excluded from the best model\n",
        "            best_optim_base_obj: the object of the best model\n",
        "            p: the radar plot of the distribution of fairness metrics for all nearly optimal models\n",
        "        \"\"\"\n",
        "        FAIM_area_list = []\n",
        "        ids = np.sum(\n",
        "            self.fairmetrics_df[targeted_metrics] < thresh_show, axis=1\n",
        "        ) == len(targeted_metrics)\n",
        "        if len(ids) == 0:\n",
        "            raise ValueError(\"The thresh is too low!\")\n",
        "        fairmetrics_df = self.fairmetrics_df.loc[ids, :]\n",
        "\n",
        "        sen_var_exclusion = fairmetrics_df[\"sen_var_exclusion\"]\n",
        "        perf = fairmetrics_df[\"auc\"]\n",
        "        df = fairmetrics_df.drop(columns=[\"sen_var_exclusion\"])\n",
        "        df = df[targeted_metrics]\n",
        "\n",
        "        print(f\"There are {df.shape[0]} models for final fairness selection.\")\n",
        "\n",
        "        if \"title\" in kwargs.keys():\n",
        "            title = kwargs[\"title\"]\n",
        "        else:\n",
        "            title = None\n",
        "        p_radar = plot_radar(df, thresh_show=thresh_show, title=title)\n",
        "        # p_radar.show()\n",
        "        p, fair_idx_df = plot_scatter(df, perf, sen_var_exclusion, title=title)\n",
        "        self.p = p\n",
        "        p.show()\n",
        "\n",
        "        for i, id in enumerate(df.index):\n",
        "            values = df.loc[id, :]\n",
        "            FAIM_area_list.append(fairarea(values))\n",
        "        ranking = np.argsort(np.argsort(FAIM_area_list))\n",
        "\n",
        "        if best_id is not None:\n",
        "            assert best_sen_exclusion is not None\n",
        "            self.best_id = best_id\n",
        "            self.best_sen_exclusion = best_sen_exclusion\n",
        "        else:\n",
        "            self.best_id = df.index[np.where(ranking == 0)][0]\n",
        "            self.best_sen_exclusion = sen_var_exclusion.iloc[np.argmin(FAIM_area_list)]\n",
        "\n",
        "        id_senario = [\n",
        "            index\n",
        "            for index, item in enumerate(list(self.coefs.keys()))\n",
        "            if item == self.best_sen_exclusion\n",
        "        ][0]\n",
        "\n",
        "        self.best_coef = (\n",
        "            self.coefs[self.best_sen_exclusion]\n",
        "            .drop(columns=[\"perf_metric\"])\n",
        "            .loc[self.best_id - self.n_final * id_senario, :]\n",
        "        )\n",
        "        self.best_optim_base_obj = self.optim_base_obj_list[self.best_sen_exclusion]\n",
        "\n",
        "        # confidence interval\n",
        "        dat_uncertainty = self.dat_train.sample(\n",
        "            n=np.min([50000, self.dat_train.shape[0]]), random_state=42\n",
        "        )\n",
        "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
        "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
        "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
        "        x_with_constant = self.data_process(\n",
        "            dat_uncertainty,\n",
        "            selected_vars=selected_vars,\n",
        "            selected_vars_cat=selected_vars_cat,\n",
        "        )[0].values\n",
        "\n",
        "        prob_train, _, _ = self.test(dat_uncertainty)\n",
        "        best_se = None\n",
        "        fisher_information = (\n",
        "            x_with_constant.T @ np.diag(prob_train * (1 - prob_train)) @ x_with_constant\n",
        "        )\n",
        "        print(\"multiplication successed!\")\n",
        "        cov = np.linalg.pinv(fisher_information)\n",
        "        best_se = [np.sqrt(cov[i, i]) for i in range(cov.shape[0])]\n",
        "\n",
        "        # self.best_thresh = self.thresh_list[self.best_id]\n",
        "        best_results = {\n",
        "            \"best_coef\": self.best_coef,\n",
        "            \"best_sen_exclusion\": self.best_sen_exclusion,\n",
        "            \"best_se\": best_se,\n",
        "            \"best_optim_base_obj\": self.best_optim_base_obj,\n",
        "        }\n",
        "\n",
        "        return best_results, fair_idx_df\n",
        "\n",
        "    def post_mitigate(self):\n",
        "        pass\n",
        "\n",
        "    def test(self, dat_test, model=None, params=None, thresh=None):\n",
        "        \"\"\"Test the best model regarding fairness\n",
        "\n",
        "        Args:\n",
        "            dat_test (data frame): the data frame of the test data\n",
        "            model (object, optional): the object of the model to be tested. Defaults to None.\n",
        "            params (optional): the parameters of the model to be tested. Defaults to None.\n",
        "            thresh (optional): the threshold of the predictions. Defaults to None.\n",
        "\n",
        "        Methods:\n",
        "        +--------------+------------+----------------------------------------+\n",
        "        | Model        | Params     | Description                            |\n",
        "        +--------------+------------+----------------------------------------+\n",
        "        | None         | None       | Test the best model produced by FAIM.  |\n",
        "        +--------------+------------+----------------------------------------+\n",
        "        | model results| None       | Test the provided model with parameters|\n",
        "        |              |            | embedded.                              |\n",
        "        +--------------+------------+----------------------------------------+\n",
        "        | model results| as required| Test the provided model with the       |\n",
        "        |              |            | parameters additionally provided.      |\n",
        "        +--------------+------------+----------------------------------------+\n",
        "\n",
        "        Returns:\n",
        "            prob_test: the predicted probabilities of the test data\n",
        "            fairmetrics: the fairness metrics of the test data\n",
        "            fairsummary: the fairness summary of the test data for each subgroup\n",
        "\n",
        "        \"\"\"\n",
        "        self.dat_test = dat_test\n",
        "        excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
        "        selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
        "        selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
        "        x_with_constant, sen_var, y_test = self.data_process(\n",
        "            dat_test, selected_vars=selected_vars, selected_vars_cat=selected_vars_cat\n",
        "        )\n",
        "\n",
        "        if model is None:\n",
        "            prob_test = self.best_optim_base_obj.model_optim.model.predict(\n",
        "                params=self.best_coef, exog=x_with_constant\n",
        "            )\n",
        "        else:\n",
        "            if isinstance(model, type(self.optim_results)) and params is None:\n",
        "                prob_test = model.predict(exog=x_with_constant)\n",
        "            elif isinstance(model, type(self.optim_model)) and params is not None:\n",
        "                prob_test = model.predict(params=params, exog=x_with_constant)\n",
        "            else:\n",
        "                raise ValueError(\"Please provide the right model!\")\n",
        "\n",
        "        thresh = find_optimal_cutoff(y_test, prob_test)[0]\n",
        "        pred_test = prob_test > thresh\n",
        "        fe = FAIMEvaluator(\n",
        "            y_true=np.array(y_test),\n",
        "            y_pred_bin=pred_test,\n",
        "            y_pred=prob_test,\n",
        "            sen_var=sen_var,\n",
        "            weighted=self.weighted,\n",
        "            weights=self.weights,\n",
        "        )\n",
        "        fairmetrics = fe.fairmetrics\n",
        "        clametrics = fe.clametrics\n",
        "\n",
        "        return prob_test, fairmetrics, clametrics\n",
        "\n",
        "\n",
        "    def explain(self, method=\"best\"):\n",
        "        \"\"\"Compute SHAP values for FAIM (best) or baseline (ori) model.\"\"\"\n",
        "\n",
        "        import shap\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        import os\n",
        "\n",
        "        # Choose model + coefficients\n",
        "        if method == \"best\":\n",
        "            coef = self.best_coef\n",
        "            excluded = set(self.best_sen_exclusion.split(\"_\"))\n",
        "            used_vars = [v for v in self.vars if v not in excluded]\n",
        "            used_vars_cat = [v for v in self.vars_cat if v not in excluded]\n",
        "            model = self.best_optim_base_obj.model_optim.model  # 18 features\n",
        "        else:  # method == \"ori\"\n",
        "            coef = self.optim_results.params\n",
        "            used_vars = self.vars\n",
        "            used_vars_cat = self.vars_cat\n",
        "            model = self.optim_model  # <-- FIX: Use the ORIGINAL model (23 features)\n",
        "\n",
        "        model_cols = model.exog_names  # Get the correct columns for THIS model\n",
        "\n",
        "        # ---- Build background data ----\n",
        "        bg_full = self.data_process(\n",
        "            self.dat_train,\n",
        "            selected_vars=used_vars,\n",
        "            selected_vars_cat=used_vars_cat,\n",
        "        )[0]\n",
        "\n",
        "        # IMPORTANT: reduce to model columns BEFORE kmeans\n",
        "        bg_full = bg_full[model_cols]\n",
        "\n",
        "        # Now safe to summarize\n",
        "        bg_data = shap.kmeans(bg_full, k=50)\n",
        "\n",
        "        # ---- Build explain data ----\n",
        "        ex_full = self.data_process(\n",
        "            self.dat_expl,\n",
        "            selected_vars=used_vars,\n",
        "            selected_vars_cat=used_vars_cat,\n",
        "        )[0]\n",
        "\n",
        "        # Also reduce ex_data to model columns\n",
        "        ex_data = ex_full[model_cols].sample(n=200, random_state=42)\n",
        "\n",
        "        # ---- Model function that ALWAYS uses correct columns ----\n",
        "        def f(X):\n",
        "            if not isinstance(X, pd.DataFrame):\n",
        "                X = pd.DataFrame(X, columns=model_cols)\n",
        "            return model.predict(params=coef, exog=X)\n",
        "\n",
        "        # ---- Run SHAP ----\n",
        "        explainer = shap.KernelExplainer(f, bg_data)\n",
        "        shap_vals = explainer.shap_values(ex_data)\n",
        "\n",
        "        # KernelExplainer output may be list\n",
        "        shap_vals = shap_vals[1] if isinstance(shap_vals, list) else shap_vals\n",
        "\n",
        "        # ---- Save ----\n",
        "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        pd.DataFrame(shap_vals, columns=model_cols).to_csv(\n",
        "            os.path.join(output_dir, f\"{method}.csv\")\n",
        "        )\n",
        "\n",
        "        return shap_vals\n",
        "\n",
        "        def f(bg):\n",
        "            model = self.best_optim_base_obj.model_optim.model\n",
        "            if method == \"best\":\n",
        "                return model.predict(params=self.best_coef, exog=bg)\n",
        "            else:\n",
        "                return model.predict(params=self.optim_results.params, exog=bg)\n",
        "\n",
        "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
        "        if method == \"best\":\n",
        "            excluded_vars = self.best_sen_exclusion.split(\"_\")\n",
        "            selected_vars = [i for i in self.vars if i not in excluded_vars]\n",
        "            selected_vars_cat = [i for i in self.vars_cat if i not in excluded_vars]\n",
        "\n",
        "        else:\n",
        "            selected_vars = self.vars\n",
        "            selected_vars_cat = self.vars_cat\n",
        "\n",
        "        bg_data = self.data_process(\n",
        "            self.dat_train,\n",
        "            selected_vars=selected_vars,\n",
        "            selected_vars_cat=selected_vars_cat,\n",
        "        )[0].sample(n=1000, random_state=42)\n",
        "        ex_data = self.data_process(\n",
        "            self.dat_expl,\n",
        "            selected_vars=selected_vars,\n",
        "            selected_vars_cat=selected_vars_cat,\n",
        "        )[0].sample(n=200, random_state=42)\n",
        "\n",
        "        e = shap.KernelExplainer(f, bg_data)\n",
        "        shap_values_train = e.shap_values(ex_data)\n",
        "        shap_values_train_1 = shap_values_train[1].squeeze()\n",
        "        pd.DataFrame(shap_values_train_1).to_csv(\n",
        "            os.path.join(output_dir, f\"{method}.csv\")\n",
        "        )\n",
        "\n",
        "        return shap_values_train_1\n",
        "\n",
        "\n",
        "    def compare_explain(self, overide=True, top_n=None):\n",
        "        \"\"\"Compare the SHAP values of the best model and original model\"\"\"\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        import textwrap\n",
        "        import os\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "\n",
        "        def add_bar_labels(ax, values, fmt=\"{:.4f}\", padding=3):\n",
        "            \"\"\"\n",
        "            Add numeric labels to horizontal bar plots.\n",
        "            \"\"\"\n",
        "            for i, v in enumerate(values):\n",
        "                ax.text(\n",
        "                    v,\n",
        "                    i,\n",
        "                    fmt.format(v),\n",
        "                    va=\"center\",\n",
        "                    ha=\"left\",\n",
        "                    fontsize=16  #  CHANGED from 10 to 16\n",
        "                )\n",
        "        def clean_spines(ax):\n",
        "            ax.spines[\"top\"].set_visible(False)\n",
        "            ax.spines[\"right\"].set_visible(False)\n",
        "            ax.spines[\"left\"].set_visible(True)\n",
        "            ax.spines[\"bottom\"].set_visible(True)\n",
        "\n",
        "        output_dir = os.path.join(self.output_dir, \"explain\")\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "\n",
        "        # ---------- LOAD SHAP VALUES ----------\n",
        "        if (not os.path.exists(os.path.join(output_dir, \"best.csv\"))) or overide:\n",
        "            shap_best = self.explain(method=\"best\")\n",
        "        else:\n",
        "            shap_best = pd.read_csv(\n",
        "                os.path.join(output_dir, \"best.csv\"), index_col=0\n",
        "            ).values  # Don't reshape - keep as 2D\n",
        "\n",
        "        if (not os.path.exists(os.path.join(output_dir, \"ori.csv\"))) or overide:\n",
        "            shap_ori = self.explain(method=\"ori\")\n",
        "        else:\n",
        "            shap_ori = pd.read_csv(\n",
        "                os.path.join(output_dir, \"ori.csv\"), index_col=0\n",
        "            ).values  # Don't reshape - keep as 2D\n",
        "\n",
        "        # ---------- AGGREGATE SHAP VALUES (mean absolute) ----------\n",
        "        # Take mean absolute SHAP value across all samples for each feature\n",
        "        shap_best_agg = np.mean(np.abs(shap_best), axis=0)\n",
        "        shap_ori_agg = np.mean(np.abs(shap_ori), axis=0)\n",
        "\n",
        "        # ---------- SORT + SELECT TOP FEATURES ----------\n",
        "        features_best = self.best_coef.index\n",
        "        features_ori = self.optim_results.params.index\n",
        "\n",
        "        # If top_n is None, use all features\n",
        "        n_best = len(features_best) if top_n is None else top_n\n",
        "        n_ori = len(features_ori) if top_n is None else top_n\n",
        "\n",
        "        shap_best_df = (\n",
        "            pd.DataFrame({\"feature\": features_best, \"shap\": shap_best_agg})\n",
        "            .sort_values(\"shap\", ascending=False)\n",
        "            .head(n_best)\n",
        "        )\n",
        "\n",
        "        shap_ori_df = (\n",
        "            pd.DataFrame({\"feature\": features_ori, \"shap\": shap_ori_agg})\n",
        "            .sort_values(\"shap\", ascending=False)\n",
        "            .head(n_ori)\n",
        "        )\n",
        "\n",
        "        # ---------- CLEAN LABELS ----------\n",
        "        def clean_label(lbl, max_len=40):\n",
        "            lbl = lbl.replace(\"_\", \" \")\n",
        "            if len(lbl) > max_len:\n",
        "                return lbl[:max_len] + \"...\"\n",
        "            return lbl\n",
        "\n",
        "        shap_best_df[\"feature\"] = shap_best_df[\"feature\"].apply(clean_label)\n",
        "        shap_ori_df[\"feature\"] = shap_ori_df[\"feature\"].apply(clean_label)\n",
        "\n",
        "        # ---------- MAKE PLOTS WITH BIGGER SIZE ----------\n",
        "        fig, axes = plt.subplots(\n",
        "            1,\n",
        "            2,\n",
        "            figsize=(22, 12),  #  INCREASED from (18, 10) to (22, 12)\n",
        "            sharex=False\n",
        "        )\n",
        "\n",
        "        # BEST MODEL\n",
        "        axes[0].barh(\n",
        "            shap_best_df[\"feature\"],\n",
        "            shap_best_df[\"shap\"]\n",
        "        )\n",
        "        axes[0].invert_yaxis()\n",
        "        axes[0].set_title(\n",
        "            \"Fairness-aware model (FAIM) - Top SHAP features\",\n",
        "            fontsize=20,  #  INCREASED from 16 to 20\n",
        "            weight=\"bold\"\n",
        "        )\n",
        "        axes[0].set_xlabel(\"Mean |SHAP value|\", fontsize=18)  #  INCREASED from 16 to 18\n",
        "        add_bar_labels(axes[0], shap_best_df[\"shap\"].values)\n",
        "        axes[0].tick_params(axis=\"y\", labelsize=16)\n",
        "        axes[0].tick_params(axis=\"x\", labelsize=14)  #  ADDED for x-axis\n",
        "        clean_spines(axes[0])\n",
        "\n",
        "        # ORIGINAL MODEL\n",
        "        axes[1].barh(\n",
        "            shap_ori_df[\"feature\"],\n",
        "            shap_ori_df[\"shap\"],\n",
        "            color='darkorange'\n",
        "        )\n",
        "        axes[1].invert_yaxis()\n",
        "        axes[1].set_title(\n",
        "            \"Fairness-unaware model (Baseline) - Top SHAP features\",\n",
        "            fontsize=20,  #  INCREASED from 16 to 20\n",
        "            weight=\"bold\"\n",
        "        )\n",
        "        axes[1].set_xlabel(\"Mean |SHAP value|\", fontsize=18)  #  INCREASED from 16 to 18\n",
        "        add_bar_labels(axes[1], shap_ori_df[\"shap\"].values)\n",
        "        axes[1].tick_params(axis=\"y\", labelsize=16)\n",
        "        axes[1].tick_params(axis=\"x\", labelsize=14)  #  ADDED for x-axis\n",
        "        clean_spines(axes[1])\n",
        "\n",
        "        # CLEAN LAYOUT\n",
        "        plt.tight_layout(pad=4)\n",
        "        plt.subplots_adjust(wspace=0.4)\n",
        "\n",
        "        # SAVE TOO\n",
        "        plt.savefig(os.path.join(output_dir, \"shap_compare.png\"), dpi=300)\n",
        "\n",
        "        return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SX-HVJjpGib"
      },
      "source": [
        "##fairness_plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuI0pd2PwSBD"
      },
      "outputs": [],
      "source": [
        "def rgb01_hex(col):\n",
        "    col_hex = [round(i * 255) for i in col]\n",
        "    col_hex = \"#%02x%02x%02x\" % tuple(col_hex)\n",
        "    return col_hex\n",
        "\n",
        "\n",
        "def compute_area(fairness_metrics):\n",
        "    n_metric = len(fairness_metrics)\n",
        "    tmp = fairness_metrics.values.flatten().tolist()\n",
        "    tmp_1 = tmp[1:] + tmp[:1]\n",
        "\n",
        "    if n_metric > 2:\n",
        "        theta_c = 2 * np.pi / n_metric\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1) * np.sin(theta_c))\n",
        "    elif n_metric == 2:\n",
        "        area = np.sum(np.array(tmp) * np.array(tmp_1))\n",
        "    else:\n",
        "        area = np.abs(tmp[0])\n",
        "\n",
        "    return area\n",
        "\n",
        "\n",
        "def plot_perf_metric(\n",
        "    perf_metric, eligible, x_range, select=None, plot_selected=False, x_breaks=None\n",
        "):\n",
        "    \"\"\" Plot performance metrics of sampled models\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        perf_metric : numpy.array or pandas.Series\n",
        "            Numeric vector of performance metrics for all sampled models\n",
        "        eligible : numpy.array or pandas.Series\n",
        "            Boolean vector of the same length of 'perf_metric', indicating \\\n",
        "                whether each sample is eligible.\n",
        "        x_range : list\n",
        "            Numeric vector indicating the range of eligible values for \\\n",
        "                performance metrics.\n",
        "            Will be indicated by dotted vertical lines in plots.\n",
        "        select : list or numpy.array, optional (default: None)\n",
        "            Numeric vector of indexes of 'perf_metric' to be selected\n",
        "        plot_selected : bool, optional (default: False)\n",
        "            Whether performance metrics of selected models should be plotted in \\\n",
        "                a secondary figure.\n",
        "        x_breaks : list, optional (default: None)\n",
        "            If selected models are to be plotted, the breaks to use in the \\\n",
        "                histogram\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        plot : plotnine.ggplot\n",
        "            Histogram(s) of model performance made using ggplot\n",
        "    \"\"\"\n",
        "    m = len(perf_metric)\n",
        "    perf_df = pd.DataFrame(perf_metric, columns=[\"perf_metric\"], index=None)\n",
        "    plot = (\n",
        "        pn.ggplot(perf_df, pn.aes(x=\"perf_metric\"))\n",
        "        + pn.geoms.geom_histogram(\n",
        "            breaks=np.linspace(np.min(perf_metric), np.max(perf_metric), 40)\n",
        "        )\n",
        "        + pn.geoms.geom_vline(xintercept=x_range, linetype=\"dashed\", size=0.7)\n",
        "        + pn.labels.labs(\n",
        "            x=\"Ratio of loss to minimum loss\",\n",
        "            title=\"\"\"Loss of {m:d} sampled models\n",
        "                \\n{n_elg:d} ({per_elg:.1f}%) sampled models are eligible\"\"\".format(\n",
        "                m=m, n_elg=np.sum(eligible), per_elg=np.sum(eligible) * 100 / m\n",
        "            ),\n",
        "        )\n",
        "        + pn.themes.theme_bw()\n",
        "        + pn.themes.theme(\n",
        "            title=pn.themes.element_text(ha=\"left\"),\n",
        "            axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
        "            axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
        "        )\n",
        "    )\n",
        "    if plot_selected:\n",
        "        if select is None:\n",
        "            print(\"'select' vector is not specified!\\nUsing all models instead\")\n",
        "            select = [i for i in range(len(perf_df))]\n",
        "        try:\n",
        "            perf_select = perf_df.iloc[select]\n",
        "        except:\n",
        "            print(\n",
        "                \"Invalid indexes detected in 'select' vector!\\nUsing all models instead\"\n",
        "            )\n",
        "            select = [i for i in range(len(perf_df))]\n",
        "            perf_select = perf_df.iloc[select]\n",
        "        plot2 = (\n",
        "            pn.ggplot(perf_select, pn.aes(x=\"perf_metric\"))\n",
        "            + pn.geoms.geom_histogram(breaks=x_breaks)\n",
        "            + pn.labels.labs(\n",
        "                x=\"Ratio of loss to minimum loss\",\n",
        "                title=\"{n_select:d} selected models\".format(n_select=len(select)),\n",
        "            )\n",
        "            + pn.themes.theme_bw()\n",
        "            + pn.themes.theme(\n",
        "                title=pn.themes.element_text(ha=\"left\"),\n",
        "                axis_title_x=pn.themes.element_text(ha=\"center\"),\n",
        "                axis_title_y=pn.themes.element_text(ha=\"center\"),\n",
        "            )\n",
        "        )\n",
        "        return (plot, plot2)\n",
        "    else:\n",
        "        return plot\n",
        "\n",
        "\n",
        "def plot_distribution(df, s=4):\n",
        "    num_metrics = df.shape[1] - 2\n",
        "    labels = df.sen_var_exclusion.unique()\n",
        "    for i in range(len(labels)):\n",
        "        if labels[i] == \"\":\n",
        "            labels[i] = \"No exclusion\"\n",
        "        elif len(labels[i].split(\"_\")) == 2:\n",
        "            labels[i] = f\"Exclusion of {' and '.join(labels[i].split('_'))}\"\n",
        "        elif len(labels[i].split(\"_\")) > 2:\n",
        "            sens = labels[i].split(\"_\")\n",
        "            labels[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
        "        else:\n",
        "            labels[i] = f\"Exclusion of {labels[i]}\"\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=num_metrics, figsize=(s * num_metrics, s))\n",
        "    for i, x in enumerate(df.columns[:-2]):\n",
        "        ax = axes[i]\n",
        "        # sns.jointplot(data=df, x=x, y=\"auc\", hue=\"sen_var_exclusion\",  ax=ax, legend=False)\n",
        "        sns.histplot(\n",
        "            data=df, x=x, hue=\"sen_var_exclusion\", bins=50, ax=ax, legend=False\n",
        "        )  # layout=(1, num_metrics), figsize=(4, 4), color=\"#595959\",\n",
        "        ax.set_title(x)\n",
        "        ax.set_xlabel(\"\")\n",
        "        ax.set_ylabel(\"Count\" if i == 0 else \"\")\n",
        "\n",
        "    plt.legend(\n",
        "        loc=\"center left\",\n",
        "        title=\"\",\n",
        "        labels=labels[::-1],\n",
        "        ncol=1,\n",
        "        bbox_to_anchor=(1.04, 0.5),\n",
        "        borderaxespad=0,\n",
        "    )\n",
        "    # plt.tight_layout() bbox_transform=fig.transFigure,\n",
        "    plt.show()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_scatter(df, perf, sen_var_exclusion, title, c1=20, c2=0.15, **kwargs):\n",
        "    ### basic settings ###\n",
        "    np.random.seed(0)\n",
        "    if \"figsize\" not in kwargs.keys():\n",
        "        fig_h = 400\n",
        "        figsize = [fig_h * df.shape[1] * 2.45 / 3, fig_h]\n",
        "    else:\n",
        "        figsize = kwargs[\"figsize\"]\n",
        "    caption_size = figsize[1] / c1  # control font size / figure size\n",
        "    fig_caption_ratio = 0.8\n",
        "    fig_font_size = caption_size * fig_caption_ratio\n",
        "\n",
        "    font_family = \"Arial\"\n",
        "    highlight_color = \"#D4AF37\"\n",
        "    fig_font_unit = c2  # control the relative position of elements\n",
        "    caption_font_unit = fig_font_unit * fig_caption_ratio\n",
        "    d = fig_font_unit / 8\n",
        "    legend_pos_y = 1 + fig_font_unit\n",
        "    subtitle_pos = [legend_pos_y + d, legend_pos_y + d + caption_font_unit]\n",
        "    xlab_pos_y = -fig_font_unit * 2\n",
        "\n",
        "    area_list = []\n",
        "    for i, id in enumerate(df.index):\n",
        "        values = df.loc[id, :]\n",
        "        area_list.append(1 / compute_area(values))\n",
        "    ranking = np.argsort(np.argsort(area_list)[::-1])\n",
        "\n",
        "    # map model id to index position (SAFE indexing)\n",
        "    id_to_pos = {df.index[i]: i for i in range(len(df))}\n",
        "\n",
        "    # jittering for display\n",
        "    jitter_control = np.zeros(len(ranking))\n",
        "    for idx in range(len(ranking)):\n",
        "        if ranking[idx] == 0:\n",
        "            jitter_control[idx] = 0\n",
        "        elif ranking[idx] <= 10 and ranking[idx] != 0:\n",
        "            jitter_control[idx] = 0.01 * np.random.uniform(0, 1)\n",
        "        elif ranking[idx] <= 10**2 and ranking[idx] > 10:\n",
        "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
        "        elif ranking[idx] <= 10**3 and ranking[idx] > 10**2:\n",
        "            jitter_control[idx] = 0.015 * np.random.uniform(0, 1)\n",
        "        else:\n",
        "            jitter_control[idx] = 0.02 * np.random.uniform(-1, 1)\n",
        "\n",
        "    ### plot ###\n",
        "    best_id = df.index[np.where(ranking == 0)][0]\n",
        "    worst_id = df.index[np.argmin(area_list)]\n",
        "    meduim_id = df.index[np.argsort(area_list)[int(len(area_list) / 2)]]\n",
        "\n",
        "    num_metrics = df.shape[1]\n",
        "    num_models = df.shape[0]\n",
        "\n",
        "    fig = make_subplots(cols=num_metrics, rows=1, horizontal_spacing=0.13)\n",
        "    cmap = sns.light_palette(\"steelblue\", as_cmap=False, n_colors=df.shape[0])\n",
        "    cmap = cmap[::-1]\n",
        "    colors = [rgb01_hex(cmap[x]) if x != 0 else highlight_color for x in ranking]\n",
        "    sizes = [10 if x != 0 else 20 for x in ranking]\n",
        "\n",
        "    shapes = sen_var_exclusion.copy().tolist()\n",
        "    cases = sen_var_exclusion.unique()\n",
        "    shapes_candidates = [\"square\", \"circle\", \"triangle-up\", \"star\"][: len(cases)]\n",
        "    for i, case in enumerate(cases):\n",
        "        for j, v in enumerate(sen_var_exclusion):\n",
        "            if v == case:\n",
        "                shapes[j] = shapes_candidates[i]\n",
        "\n",
        "        if cases[i] == \"\":\n",
        "            cases[i] = \"No exclusion\"\n",
        "        elif len(cases[i].split(\"_\")) == 2:\n",
        "            cases[i] = f\"Exclusion of {' and '.join(cases[i].split('_'))}\"\n",
        "        elif len(cases[i].split(\"_\")) > 2:\n",
        "            sens = cases[i].split(\"_\")\n",
        "            cases[i] = f\"Exclusion of {', '.join(sens[:-1])} and {sens[-1]}\"\n",
        "        else:\n",
        "            cases[i] = f\"Exclusion of {cases[i]}\"\n",
        "\n",
        "    fair_index_df = pd.DataFrame(\n",
        "        {\n",
        "            \"model id\": df.index,\n",
        "            \"fair_index\": area_list,\n",
        "            \"ranking\": ranking,\n",
        "            \"eod\": df[\"Equalized Odds\"],\n",
        "            \"colors\": colors,\n",
        "            \"shapes\": shapes,\n",
        "            \"sizes\": sizes,\n",
        "            \"cases\": sen_var_exclusion,\n",
        "            \"jitter\": jitter_control,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Add scatter plots to the subplots\n",
        "    for k, s in enumerate(shapes_candidates):\n",
        "        for i in range(num_metrics):\n",
        "            # index of sen_var_exclusion(shape) == s\n",
        "            s_idx = [idx for idx, x in enumerate(shapes) if x == s]\n",
        "            x = df.iloc[s_idx, i].values\n",
        "            js = fair_index_df.loc[fair_index_df.shapes == s, \"jitter\"].values\n",
        "            jittered_x = x + js\n",
        "\n",
        "            col = fair_index_df.loc[fair_index_df.shapes == s, \"colors\"]\n",
        "            size = fair_index_df.loc[fair_index_df.shapes == s, \"sizes\"]\n",
        "            fair_index = fair_index_df.loc[fair_index_df.shapes == s, \"fair_index\"]\n",
        "            ids = fair_index_df.loc[fair_index_df.shapes == s, \"model id\"]\n",
        "            rank_text = fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
        "            r = (\n",
        "                fair_index_df.loc[fair_index_df.shapes == s, \"ranking\"]\n",
        "                .apply(lambda x: math.log10(x + 1))\n",
        "                .values\n",
        "            )\n",
        "            sen_case = fair_index_df.loc[fair_index_df.shapes == s, \"cases\"]\n",
        "\n",
        "            hovertext = [\n",
        "                f\"Fairness index: {f:.3f}. Ranking: {x}. Model id: {i}\"\n",
        "                for f, x, i in zip(fair_index, rank_text, ids)\n",
        "            ]\n",
        "            fig.add_trace(\n",
        "                go.Scatter(\n",
        "                    x=r,\n",
        "                    y=jittered_x,\n",
        "                    customdata=hovertext,\n",
        "                    mode=\"markers\",\n",
        "                    marker=dict(\n",
        "                        color=col,\n",
        "                        symbol=s,\n",
        "                        size=size,\n",
        "                        line=dict(color=col, width=1),\n",
        "                        opacity=0.8,\n",
        "                    ),\n",
        "                    hovertemplate=\"%{customdata}.\",\n",
        "                    hoverlabel=None,\n",
        "                    hoverinfo=\"name+z\",\n",
        "                    name=cases[k],\n",
        "                ),\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "            )\n",
        "\n",
        "            if i == int((df.shape[1] + 0.5) / 2):\n",
        "                fig.update_xaxes(\n",
        "                    title_text=None,\n",
        "                    tickvals=[0, 1, 2, 3],\n",
        "                    ticktext=[1, 10, 100, 1000],\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                    tickangle=0,\n",
        "                )\n",
        "            else:\n",
        "                fig.update_xaxes(\n",
        "                    title_text=None,\n",
        "                    tickvals=[0, 1, 2, 3],\n",
        "                    ticktext=[1, 10, 100, 1000],\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                    tickangle=0,\n",
        "                )\n",
        "            fig.update_yaxes(\n",
        "                title_text=df.columns[i],\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "                showticksuffix=\"none\",\n",
        "                titlefont={\"size\": caption_size},\n",
        "            )\n",
        "\n",
        "            fig.add_vline(\n",
        "                x=0,\n",
        "                line_width=2,\n",
        "                line_dash=\"dot\",\n",
        "                line_color=highlight_color,\n",
        "                col=i + 1,\n",
        "                row=1,\n",
        "            )\n",
        "\n",
        "            min_metric = df.loc[ranking == 0, df.columns[i]].values[0]\n",
        "            max_metric = df.loc[ranking == num_models - 1, df.columns[i]].values[0]\n",
        "            meduim_metric = df.loc[\n",
        "                ranking == int(num_models / 2), df.columns[i]\n",
        "            ].values[0]\n",
        "\n",
        "            # add annotations\n",
        "            anno_size = caption_size * 0.7\n",
        "            if k == 0:\n",
        "                fig.add_hline(\n",
        "                    y=min_metric,\n",
        "                    line_width=2,\n",
        "                    line_dash=\"dot\",\n",
        "                    line_color=highlight_color,\n",
        "                    col=i + 1,\n",
        "                    row=1,\n",
        "                )\n",
        "\n",
        "                # position_y = np.mean(df.iloc[:, i])\n",
        "                min_annotation = {\n",
        "                    \"x\": 0,\n",
        "                    \"y\": min_metric,\n",
        "                    \"text\": f\"Model ID {best_id}<br> Rank No.1\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                }\n",
        "                fig.add_annotation(min_annotation, col=i + 1, row=1)\n",
        "            if meduim_id in ids:\n",
        "                medium_annotation = {\n",
        "                    \"x\": math.log10(int(num_models / 2) + 1),\n",
        "                    \"y\": meduim_metric + jitter_control[id_to_pos[meduim_id]],\n",
        "                    \"text\": f\"Model ID {meduim_id}<br> Rank No.{int(num_models/2)}\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                }\n",
        "                fig.add_annotation(medium_annotation, col=i + 1, row=1)\n",
        "            if worst_id in ids:\n",
        "                max_annotation = {\n",
        "                    \"x\": math.log10(num_models + 1),\n",
        "                    \"y\": max_metric + jitter_control[id_to_pos[worst_id]],\n",
        "                    \"text\": f\"Model ID {worst_id}<br> Rank No.{num_models}\",\n",
        "                    \"showarrow\": True,\n",
        "                    \"arrowhead\": 6,\n",
        "                    \"xanchor\": \"left\",\n",
        "                    \"yanchor\": \"bottom\",\n",
        "                    \"xref\": \"x\",\n",
        "                    \"yref\": \"y\",\n",
        "                    \"font\": {\"size\": anno_size},\n",
        "                    \"ax\": -10,\n",
        "                    \"ay\": -10,\n",
        "                    \"xshift\": 0,\n",
        "                    \"yshift\": 0,\n",
        "                    \"align\": \"left\",\n",
        "                }\n",
        "                fig.add_annotation(max_annotation, col=i + 1, row=1)\n",
        "\n",
        "    colorbar_trace = go.Scatter(\n",
        "        x=[None],\n",
        "        y=[None],\n",
        "        mode=\"markers\",\n",
        "        hoverinfo=\"none\",\n",
        "        marker=dict(\n",
        "            colorscale=[\n",
        "                rgb01_hex(np.array((243, 244, 245)) / 255),\n",
        "                \"steelblue\",\n",
        "            ],  # \"magma\",\n",
        "            showscale=True,\n",
        "            cmin=0,\n",
        "            cmax=2,\n",
        "            colorbar=dict(\n",
        "                title=None,\n",
        "                thickness=10,\n",
        "                tickvals=[0, 2],\n",
        "                ticktext=[\"Low\", \"High\"],\n",
        "                outlinewidth=0,\n",
        "                orientation=\"v\",\n",
        "                x=1,\n",
        "                y=0.5,\n",
        "            ),\n",
        "        ),\n",
        "    )\n",
        "    fig.add_trace(colorbar_trace)\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=title,\n",
        "        font=dict(family=\"Arial\", size=fig_font_size),\n",
        "        hovermode=\"closest\",\n",
        "        width=figsize[0],\n",
        "        height=figsize[1],\n",
        "        showlegend=True,\n",
        "        template=\"simple_white\",\n",
        "        legend=dict(x=0, y=legend_pos_y, orientation=\"h\"),\n",
        "    )\n",
        "\n",
        "    rectangle = {\n",
        "        \"type\": \"rect\",\n",
        "        \"x0\": -0.1,\n",
        "        \"y0\": subtitle_pos[0],\n",
        "        \"x1\": 1.1,\n",
        "        \"y1\": subtitle_pos[1],\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"fillcolor\": \"steelblue\",\n",
        "        \"opacity\": 0.1,\n",
        "    }  # 'line': {'color': 'red', 'width': 2},\n",
        "    fig.add_shape(rectangle)\n",
        "    subtitle_annotation = {\n",
        "        \"x\": -0.1,\n",
        "        \"y\": subtitle_pos[1],\n",
        "        \"text\": f\"<i> The FAIM model (i.e., fairness-aware model) is with model ID {best_id}, out of {num_models} nearly-optimal models.</i>\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": caption_size * 1.1},\n",
        "        \"align\": \"left\",\n",
        "    }\n",
        "    xaxis_annotation = {\n",
        "        \"x\": 0.5,\n",
        "        \"y\": xlab_pos_y,\n",
        "        \"text\": \"Model Rank\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": caption_size},\n",
        "    }\n",
        "    colorbar_title = {\n",
        "        \"x\": 1.05,\n",
        "        \"y\": 0.5,\n",
        "        \"text\": \"Fairness Ranking Index (FRI)\",\n",
        "        \"showarrow\": False,\n",
        "        \"xref\": \"paper\",\n",
        "        \"yref\": \"paper\",\n",
        "        \"font\": {\"size\": anno_size * 0.9},\n",
        "        \"textangle\": 90,\n",
        "    }\n",
        "    fig.add_annotation(subtitle_annotation)\n",
        "    fig.add_annotation(xaxis_annotation)\n",
        "    fig.add_annotation(colorbar_title)\n",
        "\n",
        "    for i, trace in enumerate(fig.data):\n",
        "        if i % num_metrics == 1:\n",
        "            trace.update(showlegend=True)\n",
        "        else:\n",
        "            trace.update(showlegend=False)\n",
        "    # fig.show()\n",
        "\n",
        "    return fig, fair_index_df\n",
        "\n",
        "\n",
        "def plot_radar(df, thresh_show, title, **kwargs):\n",
        "    fig = go.Figure()\n",
        "    # fig = sp.make_subplots(rows=1, cols=2)\n",
        "    cmap = sns.diverging_palette(200, 20, sep=10, s=50, as_cmap=False, n=df.shape[0])\n",
        "    theta = df.columns.tolist()\n",
        "    theta += theta[:1]\n",
        "    area_list = []\n",
        "\n",
        "    for i, id in enumerate(df.index):\n",
        "        values = df.loc[id, :]\n",
        "        area_list.append(compute_area(values))\n",
        "        values = values.values.flatten().tolist()\n",
        "        values += values[:1]\n",
        "        info = [\n",
        "            f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
        "        ]\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(\n",
        "                r=values,\n",
        "                theta=theta,\n",
        "                fill=\"toself\" if id == \"FAIReg\" else \"none\",\n",
        "                text=\"\\n\".join(info),\n",
        "                name=f\"{id}\",\n",
        "                line=dict(color=rgb01_hex(cmap[i]), dash=\"dot\"),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    ranking = np.argsort(np.argsort(area_list))\n",
        "    best_id = df.index[np.where(ranking == 0)][0]\n",
        "    print(\n",
        "        f\"The best model is No.{best_id} with metrics on validation set:\\n {df.loc[best_id, :]}\"\n",
        "    )\n",
        "    values = df.loc[best_id, :].values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "    info = [\n",
        "        f\"{theta[j]}: {v:.3f}\" for j, v in enumerate(values) if j != len(values) - 1\n",
        "    ]\n",
        "    fig.add_trace(\n",
        "        go.Scatterpolar(\n",
        "            r=values,\n",
        "            theta=theta,\n",
        "            fill=\"toself\",\n",
        "            text=\"\\n\".join(info),\n",
        "            name=f\"model {best_id}\",\n",
        "            line=dict(color=\"royalblue\", dash=\"solid\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        # title = title,\n",
        "        font=dict(family=\"Arial\", size=16),\n",
        "        polar=dict(\n",
        "            # bgcolor = \"#1e2130\",\n",
        "            radialaxis=dict(\n",
        "                showgrid=True,\n",
        "                gridwidth=1,\n",
        "                gridcolor=\"lightgray\",\n",
        "                visible=True,\n",
        "                range=[0, thresh_show],\n",
        "            )\n",
        "        ),\n",
        "        legend=dict(x=0.25, y=-0.1, orientation=\"h\"),\n",
        "        showlegend=False,\n",
        "        **kwargs,\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_bar(\n",
        "    shap_values, feature_names, original_feature_names, coef=None, title=None, **kwargs\n",
        "):\n",
        "    \"\"\"Plot the bar chart of feature importance\"\"\"\n",
        "    if \"color\" not in kwargs.keys():\n",
        "        color = \"steelblue\"\n",
        "    else:\n",
        "        color = kwargs[\"color\"]\n",
        "\n",
        "    def get_prefix(v):\n",
        "        if \"_\" in v and (v not in original_feature_names):\n",
        "            tmp = [\"_\".join(v.split(\"_\")[:i]) for i in range(len(v.split(\"_\")))]\n",
        "            return [s for s in tmp if s in original_feature_names][0]\n",
        "        else:\n",
        "            return v\n",
        "\n",
        "    if shap_values is not None:\n",
        "\n",
        "        grouped_df = pd.DataFrame({\"values\": shap_values}, index=feature_names).groupby(\n",
        "            by=get_prefix, axis=0\n",
        "        )\n",
        "        df = {k: np.mean(np.abs(g.values)) for k, g in grouped_df}\n",
        "        df = pd.DataFrame.from_dict(df, orient=\"index\").reset_index()\n",
        "        df.columns = [\"Var\", \"Value\"]\n",
        "\n",
        "        df = pd.concat(\n",
        "            [\n",
        "                df,\n",
        "                pd.DataFrame(\n",
        "                    {\n",
        "                        \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in df.Value],\n",
        "                        \"order\": np.abs(df.Value),\n",
        "                    }\n",
        "                ),\n",
        "            ],\n",
        "            axis=1,\n",
        "        )\n",
        "\n",
        "    elif coef is not None:\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"Var\": coef.index,\n",
        "                \"Value\": coef.values,\n",
        "                \"color\": [\"grey\" if i < 0 else \"steelblue\" for i in coef.values],\n",
        "                \"order\": np.abs(coef.values),\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Either shap_value or coef should be provided\")\n",
        "\n",
        "    df = df.loc[df[\"Var\"] != \"const\", :]\n",
        "    df = df.sort_values(by=\"order\", ascending=True)\n",
        "    df[\"Var\"] = pd.Categorical(df[\"Var\"], categories=df[\"Var\"].tolist(), ordered=True)\n",
        "\n",
        "    common_theme = theme(\n",
        "        text=element_text(size=24),\n",
        "        panel_grid_major_y=element_line(colour=\"lightgrey\"),\n",
        "        panel_grid_minor=element_blank(),\n",
        "        panel_background=element_blank(),\n",
        "        axis_line_x=element_line(colour=\"black\"),\n",
        "        axis_ticks_major_y=element_blank(),\n",
        "    )\n",
        "\n",
        "    x_lab = \"Feature importance\"\n",
        "\n",
        "    p = (\n",
        "        ggplot(data=df, mapping=aes(x=\"Var\", y=\"Value\", fill=\"color\"))\n",
        "        + geom_hline(yintercept=0, color=\"grey\")\n",
        "        + geom_bar(stat=\"identity\")\n",
        "        + common_theme\n",
        "        + coord_flip()\n",
        "        + labs(x=\"\", y=x_lab, title=title)\n",
        "        + theme(legend_position=\"none\")\n",
        "        + scale_fill_manual(values=[color])\n",
        "    )\n",
        "    return p"
      ]
    }
  ]
}