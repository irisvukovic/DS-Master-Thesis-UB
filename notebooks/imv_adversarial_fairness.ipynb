{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adversarial Learning - IMV Task "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MlYcPQL4ME9"
      },
      "source": [
        "## Helper function from FAIM framework for getting AUC confidence interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlxWvkdW38-5"
      },
      "outputs": [],
      "source": [
        "def get_ci_auc(y_true, y_pred, alpha=0.05, type=\"auc\"):\n",
        "    \"\"\"Calculate the confidence interval for the AUC (Area Under the Curve) score\n",
        "    or PR (Precision-Recall) score using bootstrapping.\n",
        "\n",
        "    Args:\n",
        "        y_true (array-like): True labels.\n",
        "        y_pred (array-like): Predicted scores or probabilities.\n",
        "        alpha (float, optional): Significance level for the confidence interval. Default is 0.05.\n",
        "        type (str, optional): Type of score to calculate: 'auc' (default) or 'pr' (precision-recall).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple containing the lower and upper bounds of the confidence interval.\n",
        "    \"\"\"\n",
        "\n",
        "    n_bootstraps = 1000\n",
        "    bootstrapped_scores = []\n",
        "    seed = 1234\n",
        "    np.random.seed(seed)\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    for i in range(n_bootstraps):\n",
        "        # bootstrap by sampling with replacement on the prediction indices\n",
        "        indices = rng.randint(0, len(y_pred) - 1, len(y_pred))\n",
        "\n",
        "        if len(np.unique(y_true[indices])) < 2:\n",
        "            continue\n",
        "\n",
        "        if type == \"pr\":\n",
        "            precision, recall, thresholds = precision_recall_curve(\n",
        "                y_true[indices], y_pred[indices]\n",
        "            )\n",
        "            score = auc(recall, precision)\n",
        "        else:\n",
        "            score = roc_auc_score(y_true[indices], y_pred[indices])\n",
        "        bootstrapped_scores.append(score)\n",
        "\n",
        "    sorted_scores = np.array(bootstrapped_scores)\n",
        "    sorted_scores.sort()\n",
        "\n",
        "    # 95% c.i.\n",
        "    confidence_lower = sorted_scores[int(alpha / 2 * len(sorted_scores))]\n",
        "    confidence_upper = sorted_scores[int(1 - alpha / 2 * len(sorted_scores))]\n",
        "\n",
        "    return confidence_lower, np.median(sorted_scores), confidence_upper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToD26A-Y3Vfj"
      },
      "source": [
        "### adversarial_training_framework.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code from: https://github.com/yangjenny/adversarial_learning_bias_mitigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUB8NnbdzvGo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sympy\n",
        "if not hasattr(sympy.core.numbers, 'equal_valued'):\n",
        "    sympy.core.numbers.equal_valued = lambda x, y: x == y\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve\n",
        "\n",
        "# chosen hyperparameters\n",
        "hyperparameter_list = ['learning_rate', 'num_iters', 'num_nodes', 'num_nodes_adv', 'dropout_rate', 'alpha']\n",
        "\n",
        "# get_new_control_indices is used when you want to get a specific controls:cases ratio\n",
        "# Set true when running basic model for the first time\n",
        "# set false once indices already generated for the first time\n",
        "get_new_control_indices = False\n",
        "#Set true if we want to just use data as is (i.e. don't bother with matching controls)\n",
        "use_data_as_is = True\n",
        "\n",
        "# metrics used in evaluation\n",
        "# number labels of the protected variable (k)\n",
        "def get_metrics(ypred, y, z, hyperparameters, k=7, yselect=0, eval_file=None, zpred=None):\n",
        "    metrics = dict()\n",
        "    metrics['eval_file'] = eval_file\n",
        "\n",
        "    # add hyperparameters for experiment\n",
        "    for i in range(len(hyperparameters)):\n",
        "        metrics[hyperparameter_list[i]] = hyperparameters[i]\n",
        "\n",
        "    # performance metrics\n",
        "    pred = (ypred >= 0.5).astype(int)\n",
        "\n",
        "    # Base confusion matrix\n",
        "    TN, FN, FP, TP = confusion_matrix(pred, y)\n",
        "    print(f\"TN: {TN}, FN: {FN}, FP: {FP}, TP: {TP}\")\n",
        "\n",
        "    # Standard metrics\n",
        "    metrics['accuracy'] = accuracy_score(pred, y)\n",
        "    metrics['recall'] = TP / (FN + TP + 1e-8)\n",
        "    metrics['precision'] = TP / (FP + TP + 1e-8)\n",
        "    metrics['specificity'] = TN / (TN + FP + 1e-8)\n",
        "    metrics['f1score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'] + 1e-8)\n",
        "    metrics['roc_auc'] = roc_auc_score(y, pred)\n",
        "\n",
        "    # ========================\n",
        "    # AUC with Confidence Intervals\n",
        "    # ========================\n",
        "    try:\n",
        "        auc_low, auc_median, auc_high = get_ci_auc(y, ypred, alpha=0.05, type=\"auc\")\n",
        "        metrics['auc_low'] = auc_low\n",
        "        metrics['auc'] = auc_median\n",
        "        metrics['auc_high'] = auc_high\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not calculate AUC confidence intervals: {e}\")\n",
        "        metrics['auc_low'] = np.nan\n",
        "        metrics['auc'] = metrics['roc_auc']\n",
        "        metrics['auc_high'] = np.nan\n",
        "\n",
        "    # ========================\n",
        "    #   GROUP FAIRNESS METRICS\n",
        "    # ========================\n",
        "    unique_groups = np.unique(z)\n",
        "\n",
        "    # Store per-group stats for comparisons\n",
        "    tpr_dict = {}\n",
        "    fpr_dict = {}\n",
        "    ber_dict = {}\n",
        "    acc_dict = {}\n",
        "    sr_dict = {}  # selection rate dictionary\n",
        "\n",
        "    for g in unique_groups:\n",
        "        idx = (z == g)\n",
        "        y_g = y[idx]\n",
        "        pred_g = pred[idx]\n",
        "\n",
        "        # Confusion components per group\n",
        "        TN_g, FN_g, FP_g, TP_g = confusion_matrix(pred_g, y_g)\n",
        "\n",
        "        # True Positive Rate (TPR)\n",
        "        TPR = TP_g / (TP_g + FN_g + 1e-8)\n",
        "\n",
        "        # False Positive Rate (FPR)\n",
        "        FPR = FP_g / (FP_g + TN_g + 1e-8)\n",
        "\n",
        "        # Balanced Error Rate\n",
        "        BER = 1 - 0.5 * (TPR + (1 - FPR))\n",
        "\n",
        "        # Accuracy per group\n",
        "        ACC = (TP_g + TN_g) / (TP_g + TN_g + FP_g + FN_g + 1e-8)\n",
        "\n",
        "        # Selection Rate (proportion of positive predictions)\n",
        "        SR = np.mean(pred_g)\n",
        "\n",
        "        tpr_dict[g] = TPR\n",
        "        fpr_dict[g] = FPR\n",
        "        ber_dict[g] = BER\n",
        "        acc_dict[g] = ACC\n",
        "        sr_dict[g] = SR\n",
        "\n",
        "    # Equal Opportunity difference = max TPR difference between groups\n",
        "    metrics[\"equal_opportunity_diff\"] = max(tpr_dict.values()) - min(tpr_dict.values())\n",
        "\n",
        "    # Equalized Odds difference = max(FPR_diff) + max(TPR_diff) across groups\n",
        "    metrics[\"equalized_odds_diff\"] = (max(fpr_dict.values()) - min(fpr_dict.values()) +\n",
        "                                     max(tpr_dict.values()) - min(tpr_dict.values()))\n",
        "\n",
        "    # BER equality difference\n",
        "    metrics[\"ber_equality_diff\"] = max(ber_dict.values()) - min(ber_dict.values())\n",
        "\n",
        "    # Statistical Parity (difference in selection rates between groups)\n",
        "    metrics[\"statistical_parity_diff\"] = max(sr_dict.values()) - min(sr_dict.values())\n",
        "\n",
        "    # Accuracy Equality (difference in accuracy between groups)\n",
        "    metrics[\"accuracy_equality_diff\"] = max(acc_dict.values()) - min(acc_dict.values())\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def predictmulti(prob_list):\n",
        "    ind_list = []\n",
        "    for probs in prob_list:\n",
        "        ind_list.append(np.where(probs==np.max(probs))[0][0])\n",
        "    return ind_list\n",
        "\n",
        "def confusion_matrix(ypred, y):\n",
        "    true_pos = np.sum((ypred == 1) & (y == 1))\n",
        "    true_neg = np.sum((ypred == 0) & (y == 0))\n",
        "    false_pos = np.sum((ypred == 1) & (y == 0))\n",
        "    false_neg = np.sum((ypred == 0) & (y == 1))\n",
        "    return true_neg, false_neg, false_pos, true_pos\n",
        "\n",
        "# Model class used\n",
        "class Adv_Model(object):\n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        self.method = self.params['method']\n",
        "        self.adversarial = self.method != 'basic'\n",
        "        self.num_classes = self.params['num_classes']\n",
        "        self.hyperparameters = self.params['hyperparameters']\n",
        "        self.model = self.build_model()\n",
        "        self.data = self.data_processing()\n",
        "\n",
        "    def get_indexes(self):\n",
        "        num_models = []\n",
        "        for i in range(len(hyperparameter_list)):\n",
        "            if self.adversarial:\n",
        "                # All hyperparameters are relevant for adversarial models\n",
        "                num_models.append(range(len(self.hyperparameters[hyperparameter_list[i]])))\n",
        "            else:\n",
        "                # Basic model only uses learning_rate, num_iters, num_nodes, dropout_rate\n",
        "                if hyperparameter_list[i] in ['learning_rate', 'num_iters', 'num_nodes', 'dropout_rate']:\n",
        "                    num_models.append(range(len(self.hyperparameters[hyperparameter_list[i]])))\n",
        "                else:\n",
        "                    # Placeholder so indexing doesn't break\n",
        "                    num_models.append([0])\n",
        "        return itertools.product(*num_models)\n",
        "\n",
        "    def get_hyperparameters(self, indexes):\n",
        "        hyperparameters = []\n",
        "        for i in range(len(indexes)):\n",
        "            if (i < 3 or i == 4 or self.adversarial):\n",
        "                hyperparameters.append(self.hyperparameters[hyperparameter_list[i]][indexes[i]])\n",
        "                # Print chosen hyperparameters for this run\n",
        "            else:\n",
        "                hyperparameters.append(None)\n",
        "        return hyperparameters\n",
        "\n",
        "    def params_tostring(self, indexes):\n",
        "        res = ''\n",
        "        for i in range(len(hyperparameter_list)):\n",
        "            if i > 0:\n",
        "                res += '-'\n",
        "            if (i < 3 or i == 4 or self.adversarial):\n",
        "                res += hyperparameter_list[i] + '_' + str(self.hyperparameters[hyperparameter_list[i]][indexes[i]])\n",
        "        return res\n",
        "\n",
        "    def create_dir(self, dirname):\n",
        "        if (not os.path.exists(dirname)):\n",
        "            os.makedirs(dirname)\n",
        "\n",
        "    def data_processing(self):\n",
        "        data = dict()\n",
        "        i, j = self.params['Xtrain'].shape\n",
        "        i_valid, j_valid = self.params['Xvalid'].shape\n",
        "        i_test, j_test = self.params['Xtest'].shape\n",
        "        num_nodes = self.hyperparameters['num_nodes']\n",
        "\n",
        "        data['Xtrain'] = Variable(torch.tensor(self.params['Xtrain'].values).float())\n",
        "        data['ytrain'] = Variable(torch.tensor(self.params['ytrain'].values.reshape(i, 1)).float())\n",
        "        data['Xvalid'] = Variable(torch.tensor(self.params['Xvalid'].values).float())\n",
        "        data['yvalid'] = Variable(torch.tensor(self.params['yvalid'].values.reshape(i_valid, 1)).float())\n",
        "        data['Xtest'] = Variable(torch.tensor(self.params['Xtest'].values).float())\n",
        "        data['ytest'] = Variable(torch.tensor(self.params['ytest'].values.reshape(i_test, 1)).float())\n",
        "\n",
        "        if self.num_classes > 2:\n",
        "            data['ztrain'] = Variable(torch.tensor(self.params['ztrain'].values.reshape(self.params['ztrain'].shape[0],)).long())\n",
        "            #data['zmatch'] = Variable(torch.tensor(self.params['zmatch'].values.reshape(self.params['zmatch'].shape[0],)).long())\n",
        "            data['zvalid'] = Variable(torch.tensor(self.params['zvalid'].values.reshape(self.params['zvalid'].shape[0],)).long())\n",
        "            data['ztest'] = Variable(torch.tensor(self.params['ztest'].values.reshape(self.params['ztest'].shape[0],)).long())\n",
        "        else:\n",
        "            data['ztrain'] = Variable(torch.tensor(self.params['ztrain'].values.reshape(self.params['ztrain'].shape[0],)).float())\n",
        "            #data['zmatch'] = Variable(torch.tensor(self.params['zmatch'].values.reshape(self.params['zmatch'].shape[0],)).float())\n",
        "            data['zvalid'] = Variable(torch.tensor(self.params['zvalid'].values.reshape(self.params['zvalid'].shape[0],)).float())\n",
        "            data['ztest'] = Variable(torch.tensor(self.params['ztest'].values.reshape(self.params['ztest'].shape[0],)).float())\n",
        "\n",
        "        data[\"z_group_map\"] = self.params[\"z_group_map\"]\n",
        "\n",
        "        return data\n",
        "\n",
        "    def build_model(self):\n",
        "        models = {}\n",
        "        for indexes in self.get_indexes():\n",
        "                models[indexes] = self.build_single_model(indexes)\n",
        "        return models\n",
        "\n",
        "    def build_single_model(self, indexes):\n",
        "        model = dict()\n",
        "\n",
        "        num_nodes = self.hyperparameters['num_nodes'][indexes[2]]\n",
        "        i, j = self.params['Xtrain'].shape\n",
        "        i_valid, j_valid = self.params['Xvalid'].shape\n",
        "        model['model'] = torch.nn.Sequential(\n",
        "            torch.nn.Linear(j, num_nodes),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(self.hyperparameters['dropout_rate'][indexes[4]]),\n",
        "            torch.nn.Linear(num_nodes, 1),\n",
        "            torch.nn.Sigmoid(),\n",
        "        )\n",
        "        model['loss_function'] = torch.nn.BCELoss(reduction='mean')\n",
        "        model['optimizer'] = torch.optim.Adam(model['model'].parameters(), lr=self.hyperparameters['learning_rate'][indexes[0]])\n",
        "\n",
        "        if self.adversarial:\n",
        "            num_nodes_adv = self.hyperparameters['num_nodes_adv'][indexes[3]]\n",
        "            if self.num_classes > 2:\n",
        "                num_nodes_out = self.num_classes\n",
        "            else:\n",
        "                num_nodes_out = 1\n",
        "\n",
        "            if self.adversarial and self.method == 'adv':\n",
        "                n_adv = 2\n",
        "            if (self.num_classes > 2):\n",
        "                model['adversarial_model'] = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(n_adv, num_nodes_adv),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Dropout(self.hyperparameters['dropout_rate'][indexes[4]]),\n",
        "                    torch.nn.Linear(num_nodes_adv, num_nodes_out),\n",
        "                    torch.nn.Softmax(dim=1),\n",
        "                )\n",
        "            else:\n",
        "                model['adversarial_model'] = torch.nn.Sequential(\n",
        "                    torch.nn.Linear(n_adv, num_nodes_adv),\n",
        "                    torch.nn.ReLU(),\n",
        "                    torch.nn.Dropout(self.hyperparameters['dropout_rate'][indexes[4]]),\n",
        "                    torch.nn.Linear(num_nodes_adv, num_nodes_out),\n",
        "                    torch.nn.Sigmoid(),\n",
        "                )\n",
        "            if (self.num_classes > 2):\n",
        "                model['adversarial_loss_function'] = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "            else:\n",
        "                model['adversarial_loss_function'] = torch.nn.BCELoss(reduction='mean')\n",
        "            model['adversarial_optimizer'] = torch.optim.Adam(model['adversarial_model'].parameters(), lr=self.hyperparameters['learning_rate'][indexes[0]])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self):\n",
        "        for indexes in self.get_indexes():\n",
        "            self.train_single_model(indexes)\n",
        "\n",
        "    def load_trained_models(self):\n",
        "        for indexes in self.get_indexes():\n",
        "            modelfile = 'model/model-basic.pth'\n",
        "            self.model[indexes]['model'] = torch.load(modelfile)\n",
        "            advmodelfile = 'adv/model-adv.pth'\n",
        "            self.model[indexes]['adversarial_model'] = torch.load(advmodelfile)\n",
        "\n",
        "\n",
        "    def train_single_model(self, indexes):\n",
        "        # Load in model and data\n",
        "        model = self.model[indexes]['model']\n",
        "        loss_function = self.model[indexes]['loss_function']\n",
        "        optimizer = self.model[indexes]['optimizer']\n",
        "        Xtrain = self.data['Xtrain']\n",
        "        print('original xtrain: ', Xtrain.shape)\n",
        "\n",
        "\n",
        "        Xvalid = self.data['Xvalid']\n",
        "        ytrain = self.data['ytrain']\n",
        "        yvalid = self.data['yvalid']\n",
        "        ztrain = self.data['ztrain']\n",
        "        #zmatch = self.data['zmatch']\n",
        "        zvalid = self.data['zvalid']\n",
        "\n",
        "        if use_data_as_is == False:\n",
        "            matched_cohort_indices = []\n",
        "            match_number = 20\n",
        "            idx_control = [i for i in range(len(ytrain)) if ytrain[i] == 0]\n",
        "            control_data = Xtrain[idx_control,:]\n",
        "            control_y = [ytrain[i] for i in idx_control]\n",
        "            control_z = [ztrain[i] for i in idx_control]\n",
        "            #control_age = [zmatch[i] for i in idx_control]\n",
        "            idx_case = [i for i in range(len(ytrain)) if ytrain[i] == 1]\n",
        "            case_data = Xtrain[idx_case,:]\n",
        "            case_y = [ytrain[i] for i in idx_case]\n",
        "            case_z = [ztrain[i] for i in idx_case]\n",
        "            #case_age = [zmatch[i] for i in idx_case]\n",
        "\n",
        "            if get_new_control_indices == True:\n",
        "                count = 1\n",
        "                for index in idx_case:\n",
        "                    print(str(count))\n",
        "                    patient_data = Xtrain[index,:]\n",
        "                    #patient_age = zmatch[index].numpy()\n",
        "                    patient_z = ztrain[index].numpy()\n",
        "                    age_condition = control_age == patient_age\n",
        "                    z_condition = control_z == patient_z\n",
        "                    matched_indices_bool = age_condition & z_condition\n",
        "                    matched_indices= np.array(idx_control)[matched_indices_bool]\n",
        "                    random.seed(0)\n",
        "                    random.shuffle(matched_indices)\n",
        "                    valid_indices = list(set(matched_indices)-set(matched_cohort_indices))[:match_number]\n",
        "                    matched_cohort_indices.extend(valid_indices)\n",
        "                    count=count+1\n",
        "                with open(os.path.join('control_indices_%i.pkl' % (match_number)),'wb') as f:\n",
        "                            pickle.dump(matched_cohort_indices,f)\n",
        "            else:\n",
        "                with open(os.path.join('control_indices_%i_ethnicity.pkl' % (match_number)),'rb') as f:\n",
        "                    matched_cohort_indices = pickle.load(f)\n",
        "            control_matched_data = Xtrain[matched_cohort_indices,:]\n",
        "            print('new xtrain matched control: ', control_matched_data.shape)\n",
        "            control_matched_z = [ztrain[i] for i in matched_cohort_indices]\n",
        "            control_matched_y = [ytrain[i] for i in matched_cohort_indices]\n",
        "            Xtrain = np.concatenate((control_matched_data, case_data), axis=0)\n",
        "            ytrain = np.concatenate((control_matched_y + case_y),axis=None)\n",
        "            ztrain = np.concatenate((control_matched_z + case_z),axis=None)\n",
        "\n",
        "        if self.adversarial:\n",
        "            #resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'), random_state=25)\n",
        "            resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=25)\n",
        "            Xztrain = torch.from_numpy(np.append(Xtrain, ztrain.reshape(len(ztrain), 1), axis=1))\n",
        "            Xztrain, ytrain = resample.fit_resample(Xztrain, ytrain)\n",
        "            Xtrain = torch.from_numpy(Xztrain[:,:-1])\n",
        "            ztrain = torch.from_numpy(Xztrain[:,-1])\n",
        "            ytrain = torch.from_numpy(ytrain.reshape(len(ytrain), 1))\n",
        "        else:\n",
        "            #resample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'), random_state=25)\n",
        "            resample = SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'), random_state=25)\n",
        "            Xtrain, ytrain = resample.fit_resample(Xtrain, ytrain)\n",
        "            Xtrain = torch.from_numpy(Xtrain)\n",
        "            ytrain = torch.from_numpy(ytrain.reshape(len(ytrain), 1))\n",
        "\n",
        "        if self.adversarial:\n",
        "            adversarial_model = self.model[indexes]['adversarial_model']\n",
        "            adversarial_loss_function = self.model[indexes]['adversarial_loss_function']\n",
        "            adversarial_optimizer = self.model[indexes]['adversarial_optimizer']\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Save models and metrics\n",
        "        self.create_dir('metrics/')\n",
        "        self.create_dir('model/')\n",
        "        if self.adversarial:\n",
        "            self.create_dir('adv/')\n",
        "        hyperparam_values = self.params_tostring(indexes)\n",
        "        metrics_file = 'metrics/metrics.csv'\n",
        "        metrics = []\n",
        "        modelfile = 'model/model.pth'\n",
        "        if self.adversarial:\n",
        "            advfile = 'adv/model-adv.pth'\n",
        "\n",
        "        #save training loss\n",
        "        train_loss_list = []\n",
        "        train_adversarial_loss_list = []\n",
        "        train_combined_loss_list = []\n",
        "        valid_loss_list = []\n",
        "        valid_adversarial_loss_list = []\n",
        "        valid_combined_loss_list = []\n",
        "        epoch_list = []\n",
        "\n",
        "        for t in range(self.hyperparameters['num_iters'][indexes[1]]):\n",
        "            # Forward step\n",
        "            ypred_train = model(Xtrain.float())\n",
        "            loss_train = loss_function(ypred_train, ytrain)\n",
        "\n",
        "            ypred_valid = model(Xvalid)\n",
        "            loss_valid = loss_function(ypred_valid, yvalid)\n",
        "\n",
        "            if self.adversarial:\n",
        "                if self.adversarial and self.method == 'adv':\n",
        "                    adversarial_input_train = torch.cat((ypred_train.detach(), ytrain), 1)\n",
        "                    adversarial_input_valid = torch.cat((ypred_valid.detach(), yvalid), 1)\n",
        "\n",
        "\n",
        "\n",
        "                zpred_train = adversarial_model(adversarial_input_train)\n",
        "                #add .long() if doing multiclass Z\n",
        "                adversarial_loss_train = adversarial_loss_function(zpred_train.squeeze(), ztrain.long())\n",
        "                zpred_valid = adversarial_model(adversarial_input_valid)\n",
        "                adversarial_loss_valid = adversarial_loss_function(zpred_valid.squeeze(), zvalid.long())\n",
        "\n",
        "                #ORIGINAL LOSS FUNCTION\n",
        "                combined_loss_train = loss_train - self.hyperparameters['alpha'][indexes[5]] * adversarial_loss_train\n",
        "                #CORRECTION TERM ADDED\n",
        "                #combined_loss_train = loss_train - self.hyperparameters['alpha'][indexes[5]] * adversarial_loss_train + loss_train/adversarial_loss_train\n",
        "                #PROJECTION TERM and INCREASING ALPHA WITH ITERATION ADDED\n",
        "                #combined_loss_train = loss_train - math.sqrt(t) * adversarial_loss_train + loss_train/adversarial_loss_train\n",
        "                #ORIGINAL LOSS FUNCTION\n",
        "                combined_loss_valid = loss_valid - self.hyperparameters['alpha'][indexes[5]] * adversarial_loss_valid\n",
        "                ######PROJECTION TERM ADDED\n",
        "                #combined_loss_valid = loss_valid - self.hyperparameters['alpha'][indexes[5]] * adversarial_loss_valid + loss_valid/adversarial_loss_valid\n",
        "                ######PROJECTION TERM and INCREASING ALPHA WITH ITERATION ADDED########\n",
        "                #combined_loss_valid = loss_valid - math.sqrt(t) * adversarial_loss_valid + loss_valid/adversarial_loss_valid\n",
        "\n",
        "\n",
        "            # Training log\n",
        "            if t % 100 == 0:\n",
        "                print('Iteration: {}'.format(t))\n",
        "                epoch_list.append(t)\n",
        "                if self.adversarial:\n",
        "                    print('Predictor train loss: {:.4f}'.format(loss_train))\n",
        "                    train_loss_list.append(loss_train.item())\n",
        "                    print('Predictor valid loss: {:.4f}'.format(loss_valid))\n",
        "                    valid_loss_list.append(loss_valid.item())\n",
        "                    print('Adversary train loss: {:.4f}'.format(adversarial_loss_train))\n",
        "                    train_adversarial_loss_list.append(adversarial_loss_train.item())\n",
        "                    print('Adversary valid loss: {:.4f}'.format(adversarial_loss_valid))\n",
        "                    valid_adversarial_loss_list.append(adversarial_loss_valid.item())\n",
        "                    print('Combined train loss:  {:.4f}'.format(combined_loss_train))\n",
        "                    train_combined_loss_list.append(combined_loss_train.item())\n",
        "                    print('Combined valid loss:  {:.4f}'.format(combined_loss_valid))\n",
        "                    valid_combined_loss_list.append(combined_loss_valid.item())\n",
        "                else:\n",
        "                    print('Train loss: {:.4f}'.format(loss_train))\n",
        "                    train_loss_list.append(loss_train.item())\n",
        "                    print('Valid loss: {:.4f}'.format(loss_valid))\n",
        "                    valid_loss_list.append(loss_valid.item())\n",
        "\n",
        "            # Save model\n",
        "            if t > 0 and t % 10000 == 0:\n",
        "                torch.save(model, modelfile)\n",
        "                if self.adversarial:\n",
        "                    torch.save(adversarial_model, advfile)\n",
        "\n",
        "            # Backward step\n",
        "            if self.adversarial:\n",
        "                # adv update\n",
        "                #adversarial_optimizer.zero_grad()\n",
        "                #adversarial_loss_train.backward(retain_graph=True)\n",
        "\n",
        "                # pred update\n",
        "                #optimizer.zero_grad()\n",
        "                #combined_loss_train.backward()\n",
        "                #adversarial_optimizer.step()\n",
        "\n",
        "               # 1. Update adversary\n",
        "                adversarial_optimizer.zero_grad()\n",
        "                adversarial_loss_train.backward(retain_graph=True)  # Keep graph for second backward pass\n",
        "\n",
        "                # 2. Update predictor (gradient reversal via negative sign already built in the loss)\n",
        "                optimizer.zero_grad()\n",
        "                combined_loss_train.backward()  # Can now access the retained graph\n",
        "                optimizer.step()\n",
        "                adversarial_optimizer.step()\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                loss_train.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        loss_data = {'epoch': epoch_list, 'train_loss': train_loss_list,'valid_loss': valid_loss_list, 'train_adversarial_loss': train_adversarial_loss_list, 'valid_adversarial_loss': valid_adversarial_loss_list, 'train_combined_loss': train_combined_loss_list, 'valid_combined_loss': valid_combined_loss_list}\n",
        "        loss_df = pd.DataFrame.from_dict(loss_data, orient='index')\n",
        "        loss_df = loss_df.transpose()\n",
        "        loss_df.to_csv(\"loss_metrics.csv\")\n",
        "\n",
        "        if self.adversarial:\n",
        "            plt.plot(epoch_list, train_loss_list, color='blue')\n",
        "            plt.plot(epoch_list, valid_loss_list, color='red')\n",
        "            plt.plot(epoch_list, train_adversarial_loss_list, color='blue', linestyle='--')\n",
        "            plt.plot(epoch_list, valid_adversarial_loss_list, color='red', linestyle='--')\n",
        "            plt.show()\n",
        "        else:\n",
        "            plt.plot(epoch_list, train_loss_list, color='blue')\n",
        "            plt.plot(epoch_list, valid_loss_list, color='red')\n",
        "            plt.show()\n",
        "\n",
        "        # save final model\n",
        "        torch.save(model, modelfile)\n",
        "        if self.adversarial:\n",
        "            torch.save(adversarial_model, advfile)\n",
        "\n",
        "    def evaluate(self, output_file=\"metrics.csv\"):\n",
        "        print(f\"Evaluating {len(list(self.get_indexes()))} hyperparameter combinations...\")\n",
        "\n",
        "        all_metrics = []\n",
        "\n",
        "        for indexes in self.get_indexes():\n",
        "            # Each call should return a DataFrame with exactly ONE ROW\n",
        "            df = self.evaluate_single_model(indexes)\n",
        "            all_metrics.append(df)\n",
        "\n",
        "        final_df = pd.concat(all_metrics, ignore_index=True)\n",
        "        final_df = final_df.sort_values(\"accuracy\", ascending=False)\n",
        "        final_df.to_csv(output_file, index=False)\n",
        "\n",
        "        print(f\"\\n Saved {len(final_df)} hyperparameter runs to: {output_file}\")\n",
        "\n",
        "\n",
        "    def evaluate_single_model(self, indexes):\n",
        "\n",
        "        lr = self.hyperparameters[\"learning_rate\"][indexes[0]]\n",
        "        num_iters = self.hyperparameters[\"num_iters\"][indexes[1]]\n",
        "        nodes = self.hyperparameters[\"num_nodes\"][indexes[2]]\n",
        "        dropout = self.hyperparameters[\"dropout_rate\"][indexes[4]]\n",
        "\n",
        "        print(\"================================================\")\n",
        "        print(\"Training model with hyperparameters:\")\n",
        "        print(f\"  Learning rate     : {lr}\")\n",
        "        print(f\"  Iterations        : {num_iters}\")\n",
        "        print(f\"  Predictor nodes   : {nodes}\")\n",
        "        print(f\"  Dropout           : {dropout}\")\n",
        "        print(f\"  Combination index : {indexes}\")\n",
        "\n",
        "        if self.method == \"adv\":\n",
        "            alpha = self.hyperparameters[\"alpha\"][indexes[5]]\n",
        "            nodes_adv = self.hyperparameters[\"num_nodes_adv\"][indexes[3]]\n",
        "\n",
        "            print(f\"  Alpha             : {alpha}\")\n",
        "            print(f\"  Adversary nodes   : {nodes_adv}\")\n",
        "        print(\"================================================\")\n",
        "        model = self.model[indexes]['model']\n",
        "        Xtrain = self.data['Xtrain']\n",
        "        Xvalid = self.data['Xvalid']\n",
        "        ytrain = self.data['ytrain']\n",
        "        yvalid = self.data['yvalid']\n",
        "        ztrain = self.data['ztrain']\n",
        "        zvalid = self.data['zvalid']\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        ypred_valid = model(Xvalid)\n",
        "        zpred_valid = None\n",
        "        if self.adversarial:\n",
        "            adversarial_model = self.model[indexes]['adversarial_model']\n",
        "            adversarial_model.eval()\n",
        "            if self.adversarial and self.method == 'adv':\n",
        "                adversarial_input_valid = torch.cat((ypred_valid, yvalid), 1)\n",
        "                zpred_valid = adversarial_model(adversarial_input_valid)\n",
        "\n",
        "        if zpred_valid is not None:\n",
        "            metrics_valid = pd.DataFrame(get_metrics(ypred_valid.data.numpy(), yvalid.data.numpy(), zvalid.data.numpy(), self.get_hyperparameters(indexes), k=self.num_classes, eval_file='valid_set', zpred=zpred_valid.data.numpy()), index=[0])\n",
        "        else:\n",
        "            metrics_valid = pd.DataFrame(get_metrics(ypred_valid.data.numpy(), yvalid.data.numpy(), zvalid.data.numpy(), self.get_hyperparameters(indexes), k=self.num_classes, eval_file='valid_set'), index=[0])\n",
        "        return metrics_valid\n",
        "\n",
        "\n",
        "    def group_fairness_table(self, ypred, y, z, run_label=None, output_file=None):\n",
        "        pred = (ypred >= 0.5).astype(int)\n",
        "        unique_groups = np.unique(z)\n",
        "\n",
        "        # load the map created during preprocessing\n",
        "        group_map = self.data[\"z_group_map\"]\n",
        "\n",
        "        # Fix: Create a reverse mapping from numeric codes to group names\n",
        "        numeric_to_group = {i: group_map[i] for i in group_map.keys() if i in unique_groups}\n",
        "\n",
        "        # Use the numeric groups that exist in our data\n",
        "        valid_groups = [g for g in unique_groups if g in group_map]\n",
        "\n",
        "        rows = []\n",
        "        global_selection_rate = pred.mean()\n",
        "\n",
        "        for g in unique_groups:  # Use all unique groups, not just valid ones\n",
        "            idx = (z == g)\n",
        "            y_g = y[idx]\n",
        "            pred_g = pred[idx]\n",
        "\n",
        "            if len(y_g) == 0:  # Skip empty groups\n",
        "                continue\n",
        "\n",
        "            TN, FN, FP, TP = confusion_matrix(pred_g, y_g)\n",
        "\n",
        "            TPR = TP / (TP + FN + 1e-8)\n",
        "            FPR = FP / (FP + TN + 1e-8)\n",
        "            FNR = FN / (FN + TP + 1e-8)\n",
        "            TNR = TN / (TN + FP + 1e-8)\n",
        "\n",
        "            selection_rate = pred_g.mean()\n",
        "            base_rate = y_g.mean()\n",
        "\n",
        "            sp_difference = selection_rate - global_selection_rate\n",
        "            di_ratio = (selection_rate + 1e-8) / (global_selection_rate + 1e-8)\n",
        "            ber = 0.5 * (FPR + FNR)\n",
        "\n",
        "            # Get group name if available, otherwise use numeric code\n",
        "            group_name = numeric_to_group.get(g, f\"Group_{g}\")\n",
        "\n",
        "            rows.append({\n",
        "                \"run_index\": run_label,\n",
        "                \"group_name\": group_name,\n",
        "                \"selection_rate\": selection_rate,\n",
        "                \"base_rate\": base_rate,\n",
        "                \"TPR\": TPR,\n",
        "                \"FPR\": FPR,\n",
        "                \"TNR\": TNR,\n",
        "                \"FNR\": FNR,\n",
        "                \"BER\": ber,\n",
        "                \"statistical_parity_diff\": sp_difference,\n",
        "                \"disparate_impact_ratio\": di_ratio,\n",
        "                \"n\": len(y_g)\n",
        "            })\n",
        "\n",
        "        # Create DataFrame with proper column names\n",
        "        df = pd.DataFrame(rows)\n",
        "\n",
        "        # Calculate global gap metrics\n",
        "        if len(df) > 0:\n",
        "            # Calculate gaps properly - these are the key fairness metrics\n",
        "            tpr_values = df[\"TPR\"].values\n",
        "            fpr_values = df[\"FPR\"].values\n",
        "            ber_values = df[\"BER\"].values\n",
        "\n",
        "            # Equal Opportunity difference = max(TPR) - min(TPR)\n",
        "            equal_opportunity_diff = np.max(tpr_values) - np.min(tpr_values)\n",
        "\n",
        "            # Equalized Odds difference = max(|TPR_diff| + |FPR_diff| across all group pairs)\n",
        "            # This should be calculated as the maximum difference in TPR plus maximum difference in FPR\n",
        "            tpr_diff = np.max(tpr_values) - np.min(tpr_values)\n",
        "            fpr_diff = np.max(fpr_values) - np.min(fpr_values)\n",
        "            equalized_odds_diff = tpr_diff + fpr_diff\n",
        "\n",
        "            # BER equality difference = max(BER) - min(BER)\n",
        "            ber_equality_diff = np.max(ber_values) - np.min(ber_values)\n",
        "\n",
        "            # Add these to the DataFrame as new columns\n",
        "            df[\"equal_opportunity_diff\"] = equal_opportunity_diff\n",
        "            df[\"equalized_odds_diff\"] = equalized_odds_diff\n",
        "            df[\"ber_equality_diff\"] = ber_equality_diff\n",
        "\n",
        "            # Also calculate per-group gaps for reference\n",
        "            df[\"TPR_gap\"] = equal_opportunity_diff\n",
        "            df[\"FPR_gap\"] = fpr_diff\n",
        "            df[\"BER_gap\"] = ber_equality_diff\n",
        "            df[\"equalized_odds_gap\"] = equalized_odds_diff\n",
        "\n",
        "        else:\n",
        "            # Handle empty DataFrame case\n",
        "            df[\"equal_opportunity_diff\"] = 0\n",
        "            df[\"equalized_odds_diff\"] = 0\n",
        "            df[\"ber_equality_diff\"] = 0\n",
        "            df[\"TPR_gap\"] = 0\n",
        "            df[\"FPR_gap\"] = 0\n",
        "            df[\"BER_gap\"] = 0\n",
        "            df[\"equalized_odds_gap\"] = 0\n",
        "\n",
        "        if output_file is not None:\n",
        "            df.to_csv(output_file, index=False)\n",
        "            print(f\"Saved group fairness metrics -> {output_file}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate_test_set(self):\n",
        "        print(\"Evaluating on test set...\")\n",
        "        Xtest = self.data['Xtest']\n",
        "        ytest = self.data['ytest'].numpy()\n",
        "        ztest = self.data['ztest'].numpy()\n",
        "\n",
        "        prefix = \"basic\" if self.method == \"basic\" else \"adv\"\n",
        "\n",
        "        test_metrics_file = f\"test_metrics_{prefix}.csv\"\n",
        "        test_group_file = f\"test_group_fairness_{prefix}.csv\"\n",
        "\n",
        "        all_test_metrics = []\n",
        "        all_group_metrics = []\n",
        "\n",
        "        for indexes in self.get_indexes():\n",
        "            model = self.model[indexes]['model']\n",
        "            model.eval()\n",
        "\n",
        "            ypred_test = model(Xtest).detach().numpy()\n",
        "\n",
        "            # ---------- TEST METRICS ----------\n",
        "            metrics_test = get_metrics(\n",
        "                ypred_test,\n",
        "                ytest,\n",
        "                ztest,\n",
        "                hyperparameters=self.get_hyperparameters(indexes)\n",
        "            )\n",
        "            all_test_metrics.append(metrics_test)\n",
        "\n",
        "            print(\"Test metrics:\", metrics_test)\n",
        "\n",
        "            # ---------- GROUP FAIRNESS ----------\n",
        "            df_group = self.group_fairness_table(\n",
        "                ypred_test, ytest, ztest,\n",
        "                run_label=str(indexes)\n",
        "            )\n",
        "            all_group_metrics.append(df_group)\n",
        "\n",
        "        # ---------- SAVE TEST METRICS CSV ----------\n",
        "        df_tests = pd.DataFrame(all_test_metrics)\n",
        "        df_tests.to_csv(test_metrics_file, index=False)\n",
        "        print(f\"\\n Saved test metrics to: {test_metrics_file}\")\n",
        "\n",
        "        # ---------- SAVE GROUP FAIRNESS CSV ----------\n",
        "        df_groups = pd.concat(all_group_metrics, ignore_index=True)\n",
        "        df_groups.to_csv(test_group_file, index=False)\n",
        "        print(f\" Saved group fairness metrics to: {test_group_file}\")\n",
        "\n",
        "        print(\"\\n Test evaluation complete!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQMNAhYD351M"
      },
      "source": [
        "#Load + pickle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RIPjvwg351M"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rosL_lBg351N"
      },
      "outputs": [],
      "source": [
        "cohort = pd.read_csv(\"cohort.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVkmWy5VpLm9"
      },
      "outputs": [],
      "source": [
        "cohort[\"race_sex\"] = cohort[\"race\"].astype(str) + \"_\" + cohort[\"sex\"].astype(str)\n",
        "\n",
        "cat = pd.Categorical(cohort[\"race_sex\"])\n",
        "cohort[\"Z\"] = cat.codes\n",
        "z_group_map = dict(enumerate(cat.categories))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu1qPKtpyOjg"
      },
      "outputs": [],
      "source": [
        "cohort = pd.get_dummies(cohort, columns=[\"race_sex\"])\n",
        "\n",
        "dummy_features = [col for col in cohort.columns if col.startswith(\"race_sex_\")]\n",
        "\n",
        "cohort[dummy_features] = cohort[dummy_features].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s80uMOfr351O"
      },
      "outputs": [],
      "source": [
        "# Create 70/10/20 split\n",
        "dat_train, temp_df = train_test_split(cohort, test_size=0.3, random_state=42)\n",
        "dat_expl, dat_test = train_test_split(temp_df, test_size=2/3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "PjRltf1q351P",
        "outputId": "f61083bb-f9ef-459f-9c40-9415720fe344"
      },
      "outputs": [],
      "source": [
        "vitals_cols = ['temperature','heart_rate','resp_rate','sbp','dbp','sofa_24hours']\n",
        "imputer = SimpleImputer(strategy='median') #median for continuous and numeric\n",
        "\n",
        "dat_train[vitals_cols] = imputer.fit_transform(dat_train[vitals_cols])\n",
        "dat_expl[vitals_cols] = imputer.transform(dat_expl[vitals_cols])\n",
        "dat_test[vitals_cols] = imputer.transform(dat_test[vitals_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJvfKxRz351R"
      },
      "outputs": [],
      "source": [
        "conditions = {\n",
        "    'hypertension': ['I10', 'I11', 'I12', 'I13'],\n",
        "    'congestive_heart_failure': ['I50'],\n",
        "    'copd': ['J44'],\n",
        "    'asthma': ['J45'],\n",
        "    'coronary_artery_disease': ['I25'],\n",
        "    'chronic_kidney_disease': ['N18'],\n",
        "    'diabetes': ['E10', 'E11'],\n",
        "    'connective_tissue_disease': ['M05', 'M06']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKp1i4pA351T"
      },
      "outputs": [],
      "source": [
        "y_name = 'imv'\n",
        "# these features are chosen because ???\n",
        "\n",
        "colnames = ['age', 'elective_admission',\n",
        "            'sofa_24hours', 'charlson_comorbidity_index', 'heart_rate', 'sbp', 'dbp', 'resp_rate', 'temperature'] + list(conditions.keys()) + dummy_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2XPIy8D351U"
      },
      "outputs": [],
      "source": [
        "X_train = dat_train[colnames]\n",
        "X_valid = dat_expl[colnames]\n",
        "X_test = dat_test[colnames]\n",
        "\n",
        "y_train = dat_train[y_name]\n",
        "y_valid = dat_expl[y_name]\n",
        "y_test = dat_test[y_name]\n",
        "\n",
        "Z_train = dat_train['Z']\n",
        "#Z_match = dat_train['age']\n",
        "Z_valid = dat_expl['Z']\n",
        "Z_test = dat_test['Z']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TkCj5Vf351V"
      },
      "source": [
        "  \"zmatch\": dat_train['age'] -> Given patients of the same age, are predictions still biased by race?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJyD_AlY4B0F"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "X_train.to_pickle(\"data/X_train.pkl\")\n",
        "X_valid.to_pickle(\"data/X_valid.pkl\")\n",
        "X_test.to_pickle(\"data/X_test.pkl\")\n",
        "\n",
        "y_train.to_pickle(\"data/y_train.pkl\")\n",
        "y_valid.to_pickle(\"data/y_valid.pkl\")\n",
        "y_test.to_pickle(\"data/y_test.pkl\")\n",
        "\n",
        "Z_train.to_pickle(\"data/Z_ethnicity_train.pkl\")\n",
        "#Z_match.to_pickle(\"data/Z_age_train.pkl\")\n",
        "Z_valid.to_pickle(\"data/Z_ethnicity_valid.pkl\")\n",
        "Z_test.to_pickle(\"data/Z_ethnicity_test.pkl\")\n",
        "\n",
        "\n",
        "with open(\"data/z_group_map.pkl\", \"wb\") as f:\n",
        "    pickle.dump(z_group_map, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOaDWEln3wKW",
        "outputId": "407c9403-2c6a-4c17-c56d-1c31a7bc964f"
      },
      "outputs": [],
      "source": [
        "correct_json = {\n",
        "  \"Xtrain\": \"data/X_train.pkl\",\n",
        "  \"Xvalid\": \"data/X_valid.pkl\",\n",
        "  \"Xtest\": \"data/X_test.pkl\",\n",
        "  \"ytrain\": \"data/y_train.pkl\",\n",
        "  \"yvalid\": \"data/y_valid.pkl\",\n",
        "  \"ytest\": \"data/y_test.pkl\",\n",
        "  \"ztrain\": \"data/Z_ethnicity_train.pkl\",\n",
        "  #\"zmatch\": \"data/Z_age_train.pkl\",\n",
        "  \"zvalid\": \"data/Z_ethnicity_valid.pkl\",\n",
        "  \"ztest\": \"data/Z_ethnicity_test.pkl\",\n",
        "  \"z_group_map\": \"data/z_group_map.pkl\",\n",
        "  \"method\": \"basic\",\n",
        "  \"num_classes\": 10,\n",
        "  \"hyperparameters\": {\n",
        "    \"learning_rate\": [1e-4],\n",
        "    \"num_iters\": [3000],\n",
        "    \"num_nodes\": [32],\n",
        "    \"dropout_rate\": [0.2],\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "with open(\"/content/basic_model_config.json\", \"w\") as f:\n",
        "    json.dump(correct_json, f, indent=4)\n",
        "\n",
        "print(\"UPDATED FILE CONTENT:\")\n",
        "!cat /content/basic_model_config.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKEyig_U5AxF"
      },
      "source": [
        "###trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRcagDar3drz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "class Trainer(object):\n",
        "    # modify to Trainer to accept filename directly\n",
        "    def __init__(self, config_file):\n",
        "        self.unpack_config(config_file)\n",
        "        self.load_data()\n",
        "        self.training_parameters = self.set_parameters()\n",
        "\n",
        "\n",
        "    def unpack_config(self, config_file):\n",
        "        print(\"Loading config from:\", config_file)\n",
        "        with open(config_file, \"r\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        self.Xtrain_file = config['Xtrain']\n",
        "        self.Xvalid_file = config['Xvalid']\n",
        "        self.Xtest_file = config['Xtest']\n",
        "        self.ytrain_file = config['ytrain']\n",
        "        self.yvalid_file = config['yvalid']\n",
        "        self.ytest_file = config['ytest']\n",
        "        self.ztrain_file = config['ztrain']\n",
        "        #self.zmatch_file = config['zmatch']\n",
        "        self.zvalid_file = config['zvalid']\n",
        "        self.ztest_file = config['ztest']\n",
        "        self.z_group_map_file = config['z_group_map']\n",
        "        self.method = config['method']\n",
        "        self.hyperparameters = config['hyperparameters']\n",
        "        self.num_classes = config['num_classes']\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.Xtrain_file, \"rb\") as filepath:\n",
        "            self.Xtrain = pickle.load(filepath)\n",
        "        with open(self.ytrain_file, \"rb\") as filepath:\n",
        "            self.ytrain = pickle.load(filepath)\n",
        "        with open(self.Xvalid_file, \"rb\") as filepath:\n",
        "            self.Xvalid = pickle.load(filepath)\n",
        "        with open(self.Xtest_file, \"rb\") as filepath:\n",
        "            self.Xtest = pickle.load(filepath)\n",
        "        with open(self.yvalid_file, \"rb\") as filepath:\n",
        "            self.yvalid = pickle.load(filepath)\n",
        "        with open(self.ytest_file, \"rb\") as filepath:\n",
        "            self.ytest = pickle.load(filepath)\n",
        "        with open(self.ztrain_file, \"rb\") as filepath:\n",
        "            self.ztrain = pickle.load(filepath)\n",
        "        with open(self.zvalid_file, \"rb\") as filepath:\n",
        "            self.zvalid = pickle.load(filepath)\n",
        "        #with open(self.zmatch_file, \"rb\") as filepath:\n",
        "            #self.zmatch = pickle.load(filepath)\n",
        "        with open(self.ztest_file, \"rb\") as filepath:\n",
        "            self.ztest = pickle.load(filepath)\n",
        "        with open(self.z_group_map_file, \"rb\") as f:\n",
        "            self.z_group_map = pickle.load(f)\n",
        "\n",
        "    def set_parameters(self):\n",
        "        parameters = dict()\n",
        "        parameters['Xtrain'] = self.Xtrain\n",
        "        parameters['ytrain'] = self.ytrain\n",
        "        parameters['Xvalid'] = self.Xvalid\n",
        "        parameters['yvalid'] = self.yvalid\n",
        "        parameters['Xtest'] = self.Xtest\n",
        "        parameters['ytest'] = self.ytest\n",
        "        parameters['ztrain'] = self.ztrain\n",
        "        parameters['zvalid'] = self.zvalid\n",
        "        #parameters['zmatch'] = self.zmatch\n",
        "        parameters['ztest'] = self.ztest\n",
        "        parameters['z_group_map'] = self.z_group_map\n",
        "        parameters['method'] = self.method\n",
        "        parameters['hyperparameters'] = self.hyperparameters\n",
        "        parameters['num_classes'] = self.num_classes\n",
        "        return parameters\n",
        "\n",
        "    def train(self):\n",
        "        model = Adv_Model(self.training_parameters)\n",
        "        model.train()\n",
        "        # Choose output file by method\n",
        "        if self.method == \"adv\":\n",
        "            metrics_file = \"metrics_adv.csv\"\n",
        "        else:\n",
        "            metrics_file = \"metrics_basic.csv\"\n",
        "\n",
        "        model.evaluate(output_file=metrics_file)\n",
        "        model.evaluate_test_set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3ssD0F2T3hSy",
        "outputId": "a472bd0e-bb0f-4f55-83c9-e9b9f1add185"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\"/content/basic_model_config.json\")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwnKwLViYaV2"
      },
      "source": [
        "#Pickle data for adversarial training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0_fz6LgqKKe",
        "outputId": "bacf90f0-ae8f-49f3-b8ef-15db59522fb9"
      },
      "outputs": [],
      "source": [
        "correct_json = {\n",
        "  \"Xtrain\": \"data/X_train.pkl\",\n",
        "  \"Xvalid\": \"data/X_valid.pkl\",\n",
        "  \"Xtest\": \"data/X_test.pkl\",\n",
        "  \"ytrain\": \"data/y_train.pkl\",\n",
        "  \"yvalid\": \"data/y_valid.pkl\",\n",
        "  \"ytest\": \"data/y_test.pkl\",\n",
        "  \"ztrain\": \"data/Z_ethnicity_train.pkl\",\n",
        "  #\"zmatch\": \"data/Z_age_train.pkl\",\n",
        "  \"zvalid\": \"data/Z_ethnicity_valid.pkl\",\n",
        "  \"ztest\": \"data/Z_ethnicity_test.pkl\",\n",
        "  \"z_group_map\": \"data/z_group_map.pkl\",\n",
        "  \"method\": \"adv\",\n",
        "  \"num_classes\": 10,\n",
        "  \"hyperparameters\": {\n",
        "    \"learning_rate\": [1e-4],\n",
        "    \"num_iters\": [3000],\n",
        "    \"num_nodes\": [32],\n",
        "    \"num_nodes_adv\": [16],\n",
        "    \"dropout_rate\": [0.2],\n",
        "    \"alpha\": [5]\n",
        "  }\n",
        "}\n",
        "\n",
        "with open(\"/content/adv_model_config.json\", \"w\") as f:\n",
        "    json.dump(correct_json, f, indent=4)\n",
        "\n",
        "print(\"UPDATED FILE CONTENT:\")\n",
        "!cat /content/adv_model_config.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YrVg0a_zI5HH",
        "outputId": "37c4f198-470b-4d27-a924-b0855fe7def1"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\"/content/adv_model_config.json\")\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
